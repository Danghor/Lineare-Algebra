\chapter{Lineare Abbildungen}
Der Begriff der \href{https://de.wikipedia.org/wiki/Lineare_Abbildung}{linearen Abbildungen} ist neben dem
Begriff des Vektor-Raums einer der zentralen Begriffe in der Theorie der linearen Algebra.  In diesem Kapitel
f\"{u}hren wir lineare Abbildungen ein und zeigen, wie diese durch
\href{https://de.wikipedia.org/wiki/Matrix_(Mathematik)}{Matrizen} dargestellt 
werden k\"{o}nnen.  Wir definieren Addition und Multiplikation von Matrizen und zeigen, dass bestimmte Matrizen
bez\"{u}glich der Multiplikation ein {\emph{\color{blue}Inverses}} besitzen.  Wir f\"{u}hren sogenannte
\href{https://de.wikipedia.org/wiki/Elementarmatrix}{Elementar-Matrizen} ein und demonstrieren, wie
sich mit Hilfe von Elementar-Matrizen das Inverse einer Matrix berechnen l\"{a}sst.   

Im Rest dieses Abschnittes bezeichnet $\mathbb{K}$ einen beliebigen K\"{o}rper.  In den Anwendungen wird es sich dabei
meistens um den K\"{o}rper $\mathbb{R}$ der reellen Zahlen oder den K\"{o}rper $\mathbb{C}$ der komplexen Zahlen handeln.  

\section{Definition der linearen Abbildungen}
\begin{Definition}[$\mathcal{L}(V, W)$]
Es seien $V$ und $W$ zwei $\mathbb{K}$-Vektor-R\"{a}ume.  Eine Funktion
\\[0.2cm]
\hspace*{1.3cm}
$f:V \rightarrow W$
\\[0.2cm]
ist eine {\emph{\color{blue}lineare Abbildung}} von $V$ nach $W$ genau dann, wenn Folgendes gilt:
\begin{enumerate}
\item $\forall \vec{x}, \vec{y} \in V: f(\vec{x} + \vec{y}) = f(\vec{x}) + f(\vec{y})$,

      eine lineare Abbildung ist mit der Addition vertr\"{a}glich.
\item $\forall \alpha \in \mathbb{K}: \forall \vec{x} \in V: f(\alpha \cdot \vec{x}) = \alpha \cdot f(\vec{x})$,

      eine lineare Abbildung ist auch mit der Skalar-Multiplikation vertr\"{a}glich.
\end{enumerate}
Wir bezeichnen die Menge aller linearen Abbildungen von $V$ nach $W$ mit $\mathcal{L}(V, W)$ bezeichnet, es gilt also
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{L}(V, W) := \{ f \in W^V \mid \mbox{$f$ ist lineare Abbildung} \}$.
\\[0.2cm]
Auf der Menge $\mathcal{L}(V, W)$ definieren wir eine Addition
\\[0.2cm]
\hspace*{1.3cm}
$+: \mathcal{L}(V, W) \times \mathcal{L}(V, W) \rightarrow \mathcal{L}(V, W)$
\\[0.2cm]
\emph{\color{blue}punktweise}, das hei\3t f\"{u}r $f,g \in \mathcal{L}(V, W)$ definieren wir die lineare Abbildung $f+g$,
indem wir fordern, dass f\"{u}r alle $\vec{x} \in V$
\\[0.2cm]
\hspace*{1.3cm}
$(f+g)(\vec{x}) := f(\vec{x}) + g(\vec{x})$ 
\\[0.2cm]
gilt.  Weiter definieren wir die Null-Abbildung $\mathbf{0}: V \rightarrow W$, indem wir f\"{u}r alle
$\vec{x} \in V$
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{0}(\vec{x}) := \vec{0}$
\\[0.2cm]
setzen.  Schlie\3lich definieren wir eine Skalar-Multiplikation von Elementen des K\"{o}rpers
$\mathbb{K}$ mit linearen Abbildung aus $\mathcal{L}(V,W)$.  Diese Skalar-Multiplikation bezeichnen
wir durch den Operator ``$\cdot$''.  Es ist also
\\[0.2cm]
\hspace*{1.3cm}
$\cdot: \mathbb{K} \times \mathcal{L}(V, W) \rightarrow \mathcal{L}(V, W)$
\\[0.2cm]
und wir definieren f\"{u}r alle $\alpha \in \mathbb{K}$, $f \in \mathcal{L}(V, W)$ und alle $\vec{x} \in V$
\\[0.2cm]
\hspace*{1.3cm}
$(\alpha \cdot f)(\vec{x}) := \alpha \cdot f(\vec{x})$.
 \eoxs
\end{Definition}

\exercise
Es seien $V$ und $W$ zwei $\mathbb{K}$-Vektor-R\"{a}ume und $\mathcal{L}(V, W)$ bezeichne die oben
definierte Menge der linearen Abbildungen von $V$ nach $W$.  Beweisen Sie die folgenden Behauptungen:
\begin{enumerate}[(a)]
\item Wenn $f,g \in \mathcal{L}(V,W)$ ist,  dann gilt $f+g \in \mathcal{L}(V, W)$.
\item Wenn $\alpha \in \mathbb{K}$ und $f \in \mathcal{L}(V,W)$ ist,  dann gilt  $\alpha \cdot f \in \mathcal{L}(V, W)$.
\item $\bigl\langle \langle \mathcal{L}(V, W), \mathbf{0}, + \rangle, \cdot \rangle$ ist ein $\mathbb{K}$-Vektor-Raum.
      \eoxs
\end{enumerate}

\remark
Wir k\"{o}nnen die beiden Eigenschaften, die in der Definition einer linearen Abbildung gefordert werden, zu 
einer einzigen Eigenschaft zusammenfassen, denn die Abbildung $f:V \rightarrow W$ ist genau dann linear, wenn
\\[0.2cm]
\hspace*{1.3cm}
$\forall \alpha, \beta \in \mathbb{K}: \forall \vec{x}, \vec{y} \in V: f(\alpha \cdot \vec{x} + \beta \cdot \vec{y}) = \alpha \cdot f(\vec{x}) + \beta \cdot f(\vec{y})$
\\[0.2cm]
gilt.  Weiter bemerken wir, dass f\"{u}r eine lineare Abbildung $f:V \rightarrow W$ immer
\\[0.2cm]
\hspace*{1.3cm}
$f(\vec{0}) = \vec{0}$
\\[0.2cm]
gilt, denn wir haben
\\[0.2cm]
\hspace*{1.3cm}
$f(\vec{0}) = f(0 \cdot \vec{0}) = 0 \cdot f(\vec{0}) = \vec{0}$.
\eoxs

\remark
In der Literatur wird eine lineare Abbildung $f:V \rightarrow W$ von einem Vektor-Raum $V$ in einen
Vektor-Raum $W$ auch als ein {\emph{\color{blue}Vektor-Raum-Homomorphismus}} oder k\"{u}rzer als ein
{\emph{\color{blue}Homomorphismus}} bezeichnet.  Ist die Abbildung $f$ au\3erdem noch injektiv, so hei\3t $f$ ein
{\emph{\color{blue}Monomorphismus}}.  Ist $f$ surjektiv, so sprechen wir von einem {\emph{\color{blue}Epimorphismus}} und wenn
$f$ sowohl injektiv als auch surjektiv ist, dann ist $f$ ein {\emph{\color{blue}Isomorphismus}}.  Gilt $W = V$,
hat $f$ also die Form $f: V \rightarrow V$, so nennen wir $f$ einen {\emph{\color{blue}Endomorphismus}} und wenn
sowohl $W=V$ gilt, als auch die Funktion $f$ bijektiv ist, dann ist $f$ ein {\emph{\color{blue}Automorphismus}}. 
Tabelle \ref{tab:morphism} zeigt eine Zusammenfassung dieser Definitionen.


\begin{table}[h]
  \centering
  \begin{tabular}{||l|l|l||}
    \hline
    \hline
   $f \in \mathcal{L}(V,W)$ &           & Homomorphismus \\ 
    \hline
   $f \in \mathcal{L}(V,W)$ & injektiv  & Monomorphismus \\ 
    \hline
   $f \in \mathcal{L}(V,W)$ & surjektiv & Epimorphismus \\ 
    \hline
   $f \in \mathcal{L}(V,W)$ & bijektiv  & Isomorphismus \\ 
    \hline
   $f \in \mathcal{L}(V,V)$ &           & Endomorphismus \\ 
    \hline
   $f \in \mathcal{L}(V,V)$ & bijektiv  & Automorphismus \\ 
    \hline
    \hline
  \end{tabular}
  \caption{Morphismen}
  \label{tab:morphism}
\end{table}

In der englischen Literatur wird ein  Endomorphismus $f \in \mathcal{L}(V,V)$ auch als ein
{\emph{\color{blue}Operator auf $V$}} bezeichnet.  Wir definieren zur Abk\"{u}rzung
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{L}(V) := \mathcal{L}(V,V)$.
\\[0.2cm]
Folglich bezeichnet $\mathcal{L}(V)$ die Menge der Operatoren auf dem Vektor-Raum $V$.
\eoxs

\example
Es sei $\mathbb{K}^\mathbb{N}$ der Vektor-Raum der $\mathbb{K}$-wertigen Folgen.  Wir definieren
eine Abbildung $L$, die eine gegebene Folge um das erste Element verk\"{u}rzt, die also die Folge nach
links schiebt.  Formal setzen wir
\\[0.2cm]
\hspace*{1.3cm}
$L\bigl( (x_n)_{n\in \mathbb{N}} \bigr) := (x_{n+1})_{n\in\mathbb{N}}$.
\\[0.2cm]
Weniger formal aber daf\"{u}r intuitiver k\"{o}nnten wir auch
\\[0.2cm]
\hspace*{1.3cm}
$L\bigl( (x_1, x_2, x_3, x_4, \cdots) \bigr) := (x_2, x_3, x_4, \cdots)$
\\[0.2cm]
schreiben.  Dann ist die Abbildung $L$ eine lineare Abbildung auf $\mathbb{K}^\mathbb{N}$.
\eoxs

\example
Es sei $\mathcal{C}(\mathbb{R})$ die Menge der stetigen Funktionen auf $\mathbb{R}$ und $\mathcal{D}(\mathbb{R})$ die Menge aller Funktionen
$f:\mathbb{R} \rightarrow \mathbb{R}$, die \"{u}berall differenzierbar sind und deren Ableitung stetig ist.  Definieren wir eine Funktion
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{\mathrm{d}\;}{\mathrm{d}x}: \mathcal{D}(\mathbb{R}) \rightarrow \mathcal{C}(\mathbb{R})$ \quad durch $\ds\frac{\mathrm{d}\;}{\mathrm{d}x}(f) := f'(x)$,
\\[0.2cm]
wobei $f'(x)$ die Ableitung der Funktion $f$ an der Stelle $x$ ist, so ist die Funktion $\ds\frac{\mathrm{d}\;}{\mathrm{d}x}$ eine lineare Abbildung,
denn f\"{u}r die Ableitung gilt
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{\mathrm{d}\;}{\mathrm{d}x} (\alpha \cdot f + \beta \cdot g) = \alpha \cdot \frac{\mathrm{d}\;}{\mathrm{d}x} f + \beta \cdot \frac{\mathrm{d}\;}{\mathrm{d}x} g$. \eoxs


\subsection{Kern und Bild einer linearen Abbildung}
\begin{Definition}[Kern]
  Es seien $V$ und $W$ zwei Vektor-R\"{a}ume und $f:V \rightarrow W$ sei ein lineare Abbildung.  Dann definieren wir 
  \\[0.2cm]
  \hspace*{1.3cm}
  \colorbox{red}{\framebox{\colorbox{blue}{\framebox{\colorbox{yellow}{
  $\textsl{Kern}(f) := \bigl\{ \vec{x} \in V \mid f(\vec{x}) = \vec{0} \bigr\}$.}}}}}
  \\[0.2cm] 
  Der \emph{\color{blue}Kern} einer linearen Abbildung ist also die Menge aller Vektoren aus $V$, die auf den Null-Vektor abgebildet werden.  \eoxs
\end{Definition}

\begin{Satz}
  Ist $f:V \rightarrow W$ eine lineare Abbildung, so ist $\textsl{Kern}(f)$ ein Untervektor-Raum von $V$.
\end{Satz}

\proof
Wir haben die drei Eigenschaften von Untervektor-R\"{a}umen nachzuweisen. 
\begin{enumerate}
\item Da $f$ eine lineare Abbildung ist, wissen wir, dass
      \\[0.2cm]
      \hspace*{1.3cm}
      $f\bigl(\vec{0}\bigr) = \vec{0}$
      \\[0.2cm]
      gilt.  Daraus folgt sofort, dass $\vec{0} \in \textsl{Kern}(f)$ gilt. ${\color{green}\surd}$ 
\item Seien $\vec{x}, \vec{y} \in \textsl{Kern}(f)$.  Dann haben wir nach Definition von $\textsl{Kern}(f)$
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\vec{x}) = \vec{0}$ \quad und \quad $f(\vec{y}) = \vec{0}$. 
      \\[0.2cm]
      Daraus folgt zun\"{a}chst
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\vec{x}) + f(\vec{y}) = \vec{0} + \vec{0} = \vec{0}$
      \\[0.2cm]
      und da $f$ eine lineare Abbildung ist, k\"{o}nnen wir diese Gleichung zu
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\vec{x} + \vec{y}) = \vec{0}$
      \\[0.2cm]
      umformen, was uns zeigt, dass
      \\[0.2cm]
      \hspace*{1.3cm}
      $\vec{x} + \vec{y} \in \textsl{Kern}(f)$ 
      \\[0.2cm]
      ist, so dass die Menge $\textsl{Kern}(f)$ unter der Addition abgeschlossen ist. ${\color{green}\surd}$
\item Sei $\alpha \in \mathbb{K}$ und $\vec{x} \in \textsl{Kern}(f)$.  Dann haben wir nach Definition von $\textsl{Kern}(f)$
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\vec{x}) = \vec{0}$. 
      \\[0.2cm]
      Daraus folgt zun\"{a}chst
      \\[0.2cm]
      \hspace*{1.3cm}
      $\alpha \cdot f(\vec{x}) = \alpha \cdot \vec{0} = \vec{0}$
      \\[0.2cm]
      und da $f$ eine lineare Abbildung ist, k\"{o}nnen wir diese Gleichung zu
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\alpha \cdot \vec{x}) = \vec{0}$
      \\[0.2cm]
      umformen, was uns zeigt, dass
      \\[0.2cm]
      \hspace*{1.3cm}
      $\alpha \cdot \vec{x} \in \textsl{Kern}(f)$ 
      \\[0.2cm]
      ist, so dass die Menge $\textsl{Kern}(f)$ auch unter der Skalar-Multiplikation abgeschlossen ist. ${\color{green}\surd}$
      \qeds
\end{enumerate}

\begin{Satz}
  Es seien $V$ und $W$ Vektor-R\"{a}ume und $f:V \rightarrow W$ sei eine lineare Abbildung.
  Dann ist $f$ genau dann injektiv, wenn $\textsl{Kern}(f) = \bigl\{ \vec{0} \bigr\}$ gilt.
\end{Satz}

\proof
Wir zerlegen den Beweis in zwei Teile.
\begin{enumerate}
\item[``$\Rightarrow$'':] Wir nehmen zun\"{a}chst an, dass $f$ injektiv ist.  Es ist klar, dass
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\vec{0}) = \vec{0}$
      \\[0.2cm]
      gilt und damit gilt sicher $\vec{0} \in \textsl{Kern}(f)$.  Sei nun zus\"{a}tzlich $\vec{y} \in \textsl{Kern}(f)$, es
      gelte also 
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\vec{y}) = \vec{0}$.
      \\[0.2cm]
      Da $f$ injektiv ist, folgt aus $f(\vec{y}) = \vec{0}$ und $f\bigl(\vec{0}\bigr) = \vec{0}$
      \\[0.2cm]
      \hspace*{1.3cm}
      $\vec{y} = \vec{0}$
      \\[0.2cm]
      und damit ist $\textsl{Kern}(f) = \bigl\{ \vec{0} \bigr\}$ gezeigt.
\item[``$\Leftarrow$'':] Wir setzen jetzt $\textsl{Kern}(f) = \bigl\{ \vec{0} \bigr\}$ voraus.  Falls f\"{u}r $\vec{x}, \vec{y} \in V$
      nun
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\vec{x}) = f(\vec{y})$
      \\[0.2cm]
      gilt, so k\"{o}nnen wir diese Gleichung auf Grund der Linearit\"{a}t von $f$ zu
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\vec{x} - \vec{y}) = \vec{0}$
      \\[0.2cm]
      umschreiben.  Daraus folgt aber
      \\[0.2cm]
      \hspace*{1.3cm}
      $\vec{x} - \vec{y} \in \textsl{Kern}(f)$
      \\[0.2cm]
      und da wir $\textsl{Kern}(f) = \bigl\{ \vec{0} \bigr\}$ vorausgesetzt hatten, folgt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\vec{x} - \vec{y} = 0$,
      \\[0.2cm]
      woraus wir auf $\vec{x} = \vec{y}$ schlie\3en k\"{o}nnen und das war zu zeigen.  \qeds
\end{enumerate}

\begin{Definition}[Bild]
Es seien $V$ und $W$ Vektor-R\"{a}ume und $f:V \rightarrow W$ sei eine lineare Abbildung.  Dann 
definieren wir
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{blue}{\framebox{\colorbox{yellow}{
$\textsl{Bild}(f) := \bigl\{ f(\vec{x}) \mid \vec{x} \in V \bigr\}$}}}}}
\\[0.2cm]
als die Menge der Abbilder aller Vektoren aus $V$ unter der Abbildung $f$.
\eoxs
\end{Definition}

\exercise
Zeigen Sie: Ist $f: V \rightarrow W$ eine lineare Abbildung, so ist $\textsl{Bild}(f)$ ein Untervektor-Raum von $W$.
\eox

Das Bild und der Kern einer linearen Abbildung sind \"uber den
\href{https://de.wikipedia.org/wiki/Rangsatz}{Dimensions-Satz} verkn\"upft.

\begin{Satz}[Dimensions-Satz]
  Es seien $V_1$ und $V_2$ endlich-dimensionale Vektor-R\"{a}ume und die Funktion $f: V_1 \rightarrow V_2$ sei eine lineare Abbildung.
  Dann gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  \colorbox{red}{\framebox{\colorbox{blue}{\framebox{\colorbox{yellow}{
  $\textsl{dim}(V_1) = \textsl{dim}\bigl(\textsl{Kern}(f)\bigr) + \textsl{dim}\bigl(\textsl{Bild}(f)\bigr)$.}}}}}
\end{Satz}

\proofStar
Es sei $\{ \vec{x}_1, \cdots, \vec{x}_m \}$ eine Basis von $\textsl{Kern}(f)$.  Ist $B$ eine Basis von $V_1$, so k\"{o}nnen wir mit Hilfe des
Basis-Austausch-Satzes eine Menge $W$ finden, so dass 
\\[0.2cm]
\hspace*{1.3cm}
$(B \backslash W) \cup \{ \vec{x}_1, \cdots, \vec{x}_m \}$
\\[0.2cm]
wieder eine Basis von $V_1$ ist.  Es gelte
\\[0.2cm]
\hspace*{1.3cm}
$B \backslash W = \{ \vec{y}_1, \cdots, \vec{y}_n \}$.
\\[0.2cm]
Dann ist also insgesamt die Menge
\\[0.2cm]
\hspace*{1.3cm}
$\{  \vec{x}_1, \cdots, \vec{x}_m,  \vec{y}_1, \cdots, \vec{y}_n \}$
\\[0.2cm]
eine Basis von $V_1$, so dass $\textsl{dim}(V_1) = m + n$.  Wir m\"{u}ssen daher nur noch zeigen, dass 
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{dim}\bigl(\textsl{Bild}(f)\bigr) = n$ 
\\[0.2cm] 
gilt.  Dazu zeigen wir, dass die Menge
\\[0.2cm]
\hspace*{1.3cm}
$\{ f(\vec{y}_1), \cdots, f(\vec{y}_n) \}$
\\[0.2cm]
eine Basis von $\textsl{Bild}(f)$ ist.  Hier sind zwei Sachen nachzuweisen.
\begin{enumerate}
\item Wir zeigen:
      Die Menge $\bigl\{ f(\vec{y}_1), \cdots, f(\vec{y}_n) \bigl\}$ ist ein Erzeugenden-System von $\textsl{Bild}(f)$.

      Sei $\vec{z} \in \textsl{Bild}(f)$.  Dann gibt es ein $\vec{y} \in V_1$, so dass $\vec{z} = f(\vec{y})$ ist.
      Nun ist die Menge
      \\[0.2cm]
      \hspace*{1.3cm}
      $\{ \vec{x}_1, \cdots, \vec{x}_m,  \vec{y}_1, \cdots, \vec{y}_n \}$ 
      \\[0.2cm]
      eine Basis von $V_1$. 
      Folglich muss es Skalare $\alpha_1, \cdots, \alpha_m, \beta_1, \cdots, \beta_n \in \mathbb{K}$ geben, so dass 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\alpha_1 \cdot \vec{x}_1 + \cdots + \alpha_m \cdot \vec{x}_m + \beta_1 \cdot \vec{y}_1 + \cdots + \beta_n \cdot \vec{y}_n = \vec{y}$
      \\[0.2cm]
      ist.  Wenden wir auf diese Gleichung die lineare Abbildung $f$ an und ber\"{u}cksichtigen, dass
      $f(\vec{x}_i) = \vec{0}$ ist f\"{u}r alle $i \in \{1,\cdots,m\}$, denn $\vec{x}_i \in \textsl{Kern}(f)$, so erhalten wir
      \\[0.2cm]
      \hspace*{1.3cm}
      $\beta_1 \cdot f(\vec{y}_1) + \cdots + \beta_n \cdot f(\vec{y}_n) = \vec{z}$.
      \\[0.2cm]
      Damit haben wir $\vec{z}$ aber als Linear-Kombination der Menge  $\{ f(\vec{y}_1), \cdots, f(\vec{y}_n) \}$ 
      dargestellt und das war zu zeigen.
\item Wir zeigen: Die Menge $\bigl\{ f(\vec{y}_1), \cdots, f(\vec{y}_n) \bigr\}$ ist linear unabh\"{a}ngig.
  
      Seien $\beta_1, \cdots, \beta_n \in \mathbb{K}$ und gelte
      \\[0.2cm]
      \hspace*{1.3cm}
      $\beta_1 \cdot f(\vec{y}_1) + \cdots + \beta_n \cdot f(\vec{y}_n) = \vec{0}$.
      \\[0.2cm]
      Da $f$ eine lineare Abbildung ist, k\"{o}nnen wir diese Gleichung zu
      \\[0.2cm]
      \hspace*{1.3cm}
      $f(\beta_1 \cdot \vec{y}_1 + \cdots + \beta_n \cdot \vec{y}_n) = \vec{0}$
      \\[0.2cm]
      umschreiben.  Daraus folgt aber
      \\[0.2cm]
      \hspace*{1.3cm}
      $\beta_1 \cdot \vec{y}_1 + \cdots + \beta_n \cdot \vec{y}_n \in \textsl{Kern}(f)$.
      \\[0.2cm]
      Da $\{ \vec{x}_1, \cdots, \vec{x}_m \}$ eine Basis von $\textsl{Kern}(f)$ ist, muss es also
      Skalare $\alpha_1, \cdots, \alpha_m \in \mathbb{K}$ geben, so dass
      \\[0.2cm]
      \hspace*{1.3cm}
      $\alpha_1 \cdot \vec{x}_1 + \cdots + \alpha_m \cdot \vec{x}_m = \beta_1 \cdot \vec{y}_1 + \cdots + \beta_n \cdot \vec{y}_n$
      \\[0.2cm]
      gilt.  Diese Gleichung k\"{o}nnen wir zu
      \\[0.2cm]
      \hspace*{1.3cm}
      $\alpha_1 \cdot \vec{x}_1 + \cdots + \alpha_m \cdot \vec{x}_m + (-\beta_1) \cdot \vec{y}_1 + \cdots + (-\beta_n) \cdot \vec{y}_n = \vec{0}$
      \\[0.2cm]
      umstellen.  Da die Menge $\{ \vec{x}_1, \cdots, \vec{x}_m,  \vec{y}_1, \cdots, \vec{y}_n \}$
      eine Basis von $V_1$ ist, ist diese Menge insbesondere linear unabh\"{a}ngig.  Also
      folgt aus der letzten Gleichung
      \\[0.2cm]
      \hspace*{1.3cm}
      $\alpha_1 = 0 \wedge \cdots \wedge \alpha_m = 0 \wedge -\beta_1 = 0 \wedge \cdots \wedge -\beta_n = 0$.
      \\[0.2cm]
      Damit haben also die $\beta_i$ alle den Wert $0$ und das war zu zeigen. \qeds
\end{enumerate}
\remark
Der Dimensions-Satz wird in der Literatur auch als \href{https://de.wikipedia.org/wiki/Rangsatz}{Rang-Satz} bezeichnet.
\eoxs

\exercise
Es sei $V$ ein endlich-dimensionaler $\mathbb{K}$-Vektor-Raum und es sei $f \in \mathcal{L}(V)$.
Zeigen Sie, dass $f$ genau dann surjektiv ist, wenn $f$ injektiv ist. \exend


\section{Matrizen}
Es seien $V$ und $W$ Vektor-R\"{a}ume und $f$ sei eine lineare Abbildung.  Weiter sei
$\{ \vec{x}_1, \cdots, \vec{x}_m \}$ eine Basis von $V$ und 
$\{ \vec{y}_1, \cdots, \vec{y}_n \}$ sei eine Basis von $W$.  F\"{u}r alle $j \in \{1,\cdots,m\}$ ist zun\"{a}chst
$f(\vec{x}_j)$ ein Vektor aus $W$ und da $\{ \vec{y}_1, \cdots, \vec{y}_n \}$ ein Basis von $W$ ist, 
gibt es dann eindeutig bestimmte Skalare $a_{i,j}$, so dass der Vektor $f(\vec{x}_j)$ sich als
Linear-Kombination 
\\[0.2cm]
\hspace*{1.3cm}
$f(\vec{x}_j) = \sum\limits_{i=1}^n a_{i,j} \cdot \vec{y}_i$
\\[0.2cm]
schreiben l\"{a}sst.  Ist nun allgemein $\vec{x}$ ein Vektor aus $V$, so l\"{a}sst sich $\vec{x}$ in der Form 
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} = \sum\limits_{j=1}^m \beta_j \cdot \vec{x}_j$
\\[0.2cm]
schreiben, denn $\{ \vec{x}_1, \cdots, \vec{x}_m \}$ ist ja eine Basis von $V$.  Wenden wir auf diese Gleichung die Funktion $f$ an, 
so erhalten wir
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  f(\vec{x}) & = & f\left(\sum\limits_{j=1}^m \beta_j \cdot \vec{x}_j\right)                    \\[0.5cm]
                & = & \sum\limits_{j=1}^m \beta_j \cdot f(\vec{x}_j)                               \\[0.5cm]
                & = & \sum\limits_{j=1}^m \beta_i \cdot \sum\limits_{i=1}^n a_{i,j} \cdot \vec{y}_i \\[0.5cm]
                & = &  \sum\limits_{i=1}^n \left(\sum\limits_{j=1}^m a_{i,j} \cdot \beta_j\right) \cdot \vec{y}_i. \\[0.5cm]
\end{array}
$
\\[0.2cm]
Damit sehen wir, dass lineare Abbildung $f$ in dem Moment, wo wir die Basen
$\{ \vec{x}_1, \cdots, \vec{x}_m \}$ von $V$ und $\{ \vec{y}_1, \cdots, \vec{y}_n \}$ von $W$
festgelegt haben, vollst\"{a}ndig durch die $n \cdot m$ Zahlen $\bigl(a_{i,j}\bigr)_{i=1,\cdots,n \atop j=1,\cdots,m}$ festgelegt werden.  Diese Zahlen werden 
daher zu einer $n \times m$ {\emph{\color{blue}Matrix}}
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{blue}{\framebox{\colorbox{yellow}{
$A := \left(
      \begin{array}[c]{lcl}
      a_{1,1} & \cdots & a_{1,m} \\
      \vdots & \ddots & \vdots \\
      a_{n,1} & \cdots & a_{n,m} \\
      \end{array}
      \right)
$}}}}}
\\[0.2cm]
zusammengefasst.  Die Menge aller $n \times m$ Matrizen mit Koeffizienten $a_{i,j} \in \mathbb{K}$
f\"{u}r alle $i \in \{1,\cdots,n\}$ und $j \in \{1,\cdots,m\}$ schreiben wir als $\mathbb{K}^{n \times
  m}$.

\begin{Definition}[$\mathcal{M}(f)$]
Es gelte:
\begin{enumerate}
\item $V$ und $W$ sind $\mathbb{K}$-Vektor-R\"{a}ume,
\item die Vektoren $\vec{x}_1, \cdots, \vec{x}_m$ bilden eine Basis von $V$,
\item die Vektoren $\vec{y}_1, \cdots, \vec{y}_n$ bilden eine Basis  von $W$,
\item $f \in \mathcal{L}(V, W)$.
\end{enumerate}
Dann bezeichnen wir die oben definierte Matrix $A \in \mathbb{K}^{n \times m}$ 
mit $\mathcal{M}(f)$, wir setzen also
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{M}(f) := A$. 
\eoxs
\end{Definition}

\remark
Eigentlich ist die Schreibweise $\mathcal{M}(f)$ nur dann eindeutig, wenn die Vektoren
$\vec{x}_1, \cdots, \vec{x}_m$ und $\vec{y}_1, \cdots, \vec{y}_n$ festgelegt sind.  Es
gibt Autoren, die deswegen die Notation
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{M}(f;\vec{x}_1, \cdots, \vec{x}_m; \vec{y}_1, \cdots, \vec{y}_n)$
\quad an Stelle von \quad $\mathcal{M}(f)$
\\[0.2cm]
schreiben.  Diese Schreibweise ist aber sehr schwerf\"{a}llig.  Wir
setzen daher im Folgenden voraus, dass immer, wenn die Funktion $\mathcal{M}$ verwendet wird, 
klar ist, auf Basis welcher Vektoren $\vec{x}_1, \cdots, \vec{x}_m$ und $\vec{y}_1, \cdots, \vec{y}_n$  
der Ausdruck $\mathcal{M}(f)$ definiert ist.

Zus\"{a}tzlich ist noch die folgende Spitzfindigkeit zu beachten:  Wenn wir sagen, dass 
$B = \{\vec{x}_1, \cdots, \vec{x}_m \}$ eine Basis von $V$ ist, dann ist die Reihenfolge der
Vektoren nicht festgelegt, denn eine Basis ist eine Menge und dort gibt es keine
Reihenfolge.  Daher ist die oben definierte Abbildung $\mathcal{M}: \mathcal{L}(V,W) \rightarrow\mathbb{K}^{n \times m}$ 
streng genommen nur dann eindeutig definiert, wenn wir die Reihenfolge der Vektoren in den
beteiligten Basen fixieren. 
Wir setzen im Folgenden voraus, dass es in den Anwendungen von $\mathcal{M}$ immer eine kanonische
Reihenfolge der Elemente der beiden beteiligten Basen gibt, auf die wir uns bei der Definition von
$\mathcal{M}(f)$ dann beziehen k\"{o}nnen.
\eox


 Ist $A \in \mathbb{K}^{n \times m}$ eine Matrix und ist 
\\[0.5cm]
\hspace*{1.3cm}
$\vec{x} = \left(
      \begin{array}[c]{c}
      x_{1}  \\
      \vdots \\
      x_{m}  \\
      \end{array}
    \right) \in \mathbb{K}^m
$,
\\[0.5cm]
so definieren wir das {\emph{\color{blue}Produkt}} $A \cdot \vec{x}$ als den Vektor
\\[0.5cm]
\hspace*{1.3cm}
$\vec{y} := A \cdot \vec{x} = \left(
      \begin{array}[c]{c}
      \sum\limits_{j=1}^m a_{1,j} \cdot x_{j}  \\
      \vdots \\
      \sum\limits_{j=1}^m a_{i,j} \cdot x_{j}  \\
      \vdots \\
      \sum\limits_{j=1}^m a_{n,j} \cdot x_{j}  \\
      \end{array}
      \right). 
$
\\[0.5cm]
Zur Berechnung der $i$-ten Komponente des Vektors $A \cdot \vec{x}$ wird der Vektor $\vec{x}$
also komponentenweise mit der $i$-ten Zeile der Matrix $A$ multipliziert und diese Produkte werden
dann aufaddiert.

\remark
Wir hatten die Vektoren aus $\mathbb{K}^n$ urspr\"{u}nglich als Listen der Form $\vec{x} = [x_1, \cdots, x_n]$ mit
$x_i \in \mathbb{K}$ f\"{u}r alle $i\in\{1,\cdots,n\}$ definiert.  Diese Schreibweise ist wesentlich
platzsparender als die Schreibweise
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} = \left(
      \begin{array}[c]{c}
      x_{1}  \\
      \vdots \\
      x_{m}  \\
      \end{array}
      \right),
$
\\[0.2cm]
bei der wir $\vec{x}$ als {\emph{\color{blue}Spalten-Vektor}} bezeichnen.  Im Zusammenhang mit der
Multiplikation von Matrizen mit Vektoren ist die Spaltenschreibweise allerdings 
suggestiver, so dass wir Vektoren in Zusammenhang mit der Matrizen-Multiplikation in der
Spaltenschreibweise darstellen.
\eoxs 
\pagebreak

\exercise
Es sei $A \in \mathbb{K}^{n \times m}$.  Zeigen Sie, dass die Abbildung
\\[0.2cm]
\hspace*{1.3cm}
$f: \mathbb{K} ^m \rightarrow \mathbb{K}^n$,
\\[0.2cm]
die f\"{u}r alle $\vec{x} \in \mathbb{K}^m$ durch
\\[0.2cm]
\hspace*{1.3cm}
$f(\vec{x}) := A \cdot \vec{x}$
\\[0.2cm]
definiert ist, eine lineare Abbildung ist.
\exend


\subsection{Addition und Skalar-Multiplikation von Matrizen}
Es seien $A,B \in \mathbb{K}^{n \times m}$ und in Komponentenschreibweise gelte 
\\[0.2cm]
\hspace*{1.3cm}
$A = (a_{i,j})_{1\leq i \leq n \atop 1 \leq j \leq m}$ \quad und \quad
$B = (b_{i,j})_{1\leq i \leq n \atop 1 \leq j \leq m}$.  
\\[0.2cm]
Dann definieren wir die Summe $C := A + B$ so, dass
\\[0.2cm]
\hspace*{1.3cm}
 $C = (a_{i,j} + b_{i,j})_{1\leq i \leq n \atop 1 \leq j \leq m}$ 
\\[0.2cm]
gilt, die Komponenten der
Matrix $C$ werden also durch Addition der Komponenten von $A$ und $B$  definiert.  Mit dieser
Definition ist leicht zu sehen, dass f\"{u}r beliebige Vektoren $\vec{x} \in \mathbb{K}^m$ die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$(A + B) \cdot \vec{x} = A \cdot \vec{x} + B \cdot \vec{x}$
\\[0.2cm]
gilt.  Dieses Distributiv-Gesetz ist der Grund daf\"{u}r, dass die Addition von Matrizen wie oben
angegeben definiert wird.


F\"{u}r Matrizen l\"{a}sst sich au\3erdem eine Skalar-Multiplikation definieren:  Ist $\alpha \in \mathbb{K}$
und ist $A \in \mathbb{K}^{n \times m}$, wobei
\\[0.2cm]
\hspace*{1.3cm}
$A = (a_{i,j})_{1\leq i \leq n \atop 1 \leq j \leq m}$
\\[0.2cm]
gilt, so definieren wir die $n \times m$ Matrix 
$C := \alpha \cdot A$ so, dass in Komponentenschreibweise
\\[0.2cm]
\hspace*{1.3cm}
$C = (\alpha \cdot a_{i,j})_{1\leq i \leq n \atop 1 \leq j \leq m}$ 
\\[0.2cm]
gilt.  Mit
dieser Definition gilt f\"{u}r beliebige Vektoren $\vec{x} \in \mathbb{K}^m$ die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\alpha \cdot (A \cdot \vec{x}) = (\alpha \cdot A) \cdot \vec{x} = A \cdot (\alpha \cdot \vec{x})$.
\\[0.2cm]
Diese Gleichungen zeigen, dass die Skalar-Multiplikation f\"{u}r Matrizen mit der Multiplikation einer
Matrix mit einem Vektor vertr\"{a}glich ist.

\remark
Nachdem wir die Addition und Skalar-Multiplikation von Matrizen definiert haben, ist leicht
nachzurechnen, dass die Menge $\mathbb{K}^{n \times m}$ der $n \times m$ Matrizen mit diesen
Operationen zu einem Vektor-Raum wird.  Das neutrale Element bez\"{u}glich der Addition ist die
Null-Matrix, deren s\"{a}mtliche Komponenten den Wert $0$ haben.

\remark
Sind $V$ und $W$ zwei endlich-dimensionale Vektor-R\"{a}ume mit Basen
$\{\vec{x}_1, \cdots, \vec{x}_m\}$ und $\{\vec{y}_1, \cdots, \vec{y}_n\}$ und sind $f, g \in \mathcal{L}(V, W)$ und ist $\alpha \in \mathbb{K}$, so gilt
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{M}(f+g) := \mathcal{M}(f) + \mathcal{M}(g)$ \quad und \quad
$\mathcal{M}(\alpha \cdot f) := \alpha \cdot \mathcal{M}(f)$.
\\[0.2cm]
Diese beiden Eigenschaften motivieren nachtr\"{a}glich sowohl die Definition der Addition von Matrizen
als auch die Definition der Multiplikation eines Skalars mit einer Matrizen.  
Sie zeigen, dass $\mathcal{M}$ eine lineare Abbildung des Vektor-Raums $\mathcal{L}(V,W)$ in den Vektor-Raum $\mathbb{K}^{m \times n}$ ist.  
Es handelt sich bei dieser Abbildung um einen Isomorphismus.
\eoxs



\subsection{Matrizen-Multiplikation}
Sind $l,m,n \in \mathbb{N}$ und sind $A \in \mathbb{K}^{m \times l}$ und $B \in \mathbb{K}^{n \times m}$
Matrizen, so k\"{o}nnen wir zwei lineare Abbildungen 
\\[0.2cm]
\hspace*{1.3cm}
$f : \mathbb{K}^l \rightarrow \mathbb{K}^m$  \quad und \quad $g: \mathbb{K}^m\rightarrow\mathbb{K}^n$
\\[0.2cm]
durch die Festlegung
\\[0.2cm]
\hspace*{1.3cm}
$f(\vec{x}) := A \cdot \vec{x}$ \quad f\"{u}r alle~$\vec{x} \in \mathbb{K}^l$,
 \quad und \quad
 $g(\vec{y}) := B \cdot \vec{y}$ \quad f\"{u}r alle~$\vec{y} \in \mathbb{K}^m$,
 \\[0.2cm]
definieren.  Da der Werte-Bereich der Funktion $f$ eine Teilmenge des Definitions-Bereichs der
Funktion $g$ ist, k\"{o}nnen wir die beiden Funktionen zu einer neuen Funktion $h$ verkn\"{u}pfen, indem wir
\\[0.2cm]
\hspace*{1.3cm}
$h(\vec{x}) := (g \circ f)(\vec{x}) = g(f(\vec{x}))$ \quad f\"{u}r alle~$\vec{x} \in \mathbb{K}^l$,
\\[0.2cm]
definieren.  Sie k\"{o}nnen leicht nachweisen, dass die Funktion $h$ wieder eine lineare Funktion ist
und dass 
\\[0.2cm]
\hspace*{1.3cm}
$h:\mathbb{K}^l \rightarrow \mathbb{K}^n$
\\[0.2cm]
gilt.  Folglich muss sich die Funktion $h$ ebenfalls durch eine $l \times n$ Matrix $C$ darstellen
lassen, die wir nun berechnen wollen.  Es gilt
\\[0.5cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
   h(\vec{x}) & = & g(f(\vec{x})) \\[0.2cm]
                 & = & B \cdot (A \cdot \vec{x}) \\[0.2cm]
                 & = & B \cdot \left(\begin{array}[c]{c}
                                     \vdots                          \\
                                     \sum\limits_{j=1}^l a_{i,j} \cdot x_{j}  \\
                                     \vdots                          \\
                                     \end{array}
                               \right)                                  \\[1.2cm]
                 & = & \left(\begin{array}[c]{c}
                            \vdots \\
                            \sum\limits_{k=1}^m b_{i,k} \cdot \left(\sum\limits_{j=1}^l a_{k,j} \cdot x_{j}\right)  \\
                            \vdots \\
                            \end{array}
                       \right)                                          \\[1.2cm]
                 & = & \left(\begin{array}[c]{c}
                            \vdots \\
                            \sum\limits_{j=1}^l  \left(\sum\limits_{k=1}^m b_{i,k} \cdot a_{k,j}\right) \cdot x_{j}  \\
                            \vdots \\
                            \end{array}
                       \right).                                          
\end{array}
$
\\[0.2cm]
Definieren wir also die Komponenten der Matrix $C = (c_{i,j})_{1\leq i \leq n \atop 1 \leq j \leq l}$ als
\\[0.2cm]
\hspace*{1.3cm}
$c_{i,j} := \sum\limits_{k=1}^m b_{i,k} \cdot a_{k,j}$ \quad f\"{u}r alle $i \in \{1,\cdots,n\}$ und $j \in\{1,\cdots,l\}$,
\\[0.2cm]
so sehen wir, dass diese Matrix dasselbe bewirkt, wie die sukzessive Anwendung der Matrizen $A$ und
$B$.  Daher definieren wir f\"{u}r eine 
$n \times m$ Matrix $B \in \mathbb{K}^{n \times m}$ der Form $B = (b_{i,k})_{1\leq i \leq n \atop 1 \leq j \leq m}$ und eine $m \times l$ Matrix
$A \in \mathbb{K}^{m \times l}$ der Form $A = (a_{k,j})_{1\leq i \leq m \atop 1 \leq j \leq l}$ die Produkt-Matrix $C= B \cdot A$ als die
Matrix
\\[0.2cm]
\hspace*{1.3cm}
$C = (c_{i,j})_{1\leq i \leq n \atop 1 \leq j \leq l}$, 
\\[0.2cm]
deren Komponenten durch die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$c_{i,j} := \sum\limits_{k=1}^m b_{i,k} \cdot a_{k,j}$ \quad f\"{u}r alle $i \in \{1,\cdots,n\}, j \in\{1,\cdots,l\}$,
\\[0.2cm]
gegeben sind.

\remark
Sind $V_1$, $V_2$ und $V_3$ Vektor-R\"{a}ume und gilt
\\[0.2cm]
\hspace*{1.3cm}
 $f \in \mathcal{L}(V_1, V_2)$ und $g \in \mathcal{L}(V_2, V_3)$,
\\[0.2cm]
dann haben wir die 
\href{http://de.wikipedia.org/wiki/Matrizenmultiplikation}{Matrizen-Multiplikation} gerade so definiert, dass
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{M}(g \circ f) = \mathcal{M}(g) \cdot \mathcal{M}(f)$
\\[0.2cm]
gilt.  \eoxs

\exercise
Zeigen Sie, dass f\"{u}r die Matrizen-Multiplikation das Assoziativ-Gesetz gilt.
\eoxs

\exercise
Beweisen oder widerlegen Sie, dass die Matrizen-Multiplikation kommutativ ist.
\eox

Unser Ziel im Rest dieses Abschnittes ist es zu zeigen, dass bestimmte \emph{quadratische Matrizen} ein
Inverses besitzen.  Dabei nennen wir eine Matrix $A \in \mathbb{K}^{m \times n}$ {\emph{\color{blue}quadratisch}}
falls $m = n$ ist.  Um f\"{u}r die Matrizen-Multiplikation ein neutrales Element definieren zu k\"{o}nnen,
definieren wir zun\"{a}chst das nach 
\href{http://de.wikipedia.org/wiki/Leopold_Kronecker}{Leopold Kronecker} benannte \emph{Kronecker-Delta}.


\begin{Definition}[Kronecker-Delta]
  F\"{u}r $i,j \in \mathbb{N}$ definieren wir
  \\[0.2cm]
  \hspace*{1.3cm}
  $\delta_{i,j} := \left\{
                  \begin{array}[c]{ll}
                    1 & \mbox{falls $i = j$;} \\
                    0 & \mbox{falls $i \not= j$}.
                  \end{array}
                  \right.
  $
\eoxs
\end{Definition}

Damit sind wir in der Lage, f\"{u}r alle $n \in \mathbb{N}$ eine {\emph{\color{blue}Einheits-Matrix}} $\mathrm{E}_n$ zu
definieren.  Wir definieren $\mathrm{E}_n \in \mathbb{K}^{n \times n}$ als die Matrix, die nur auf der
fallenden Diagonale mit Einsen besetzt ist, alle anderen Eintr\"{a}ge sind Null.  Formal
gilt
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{E}_n = \bigl(\delta_{i,j}\bigr)_{i=1,\cdots,n \atop j=1,\cdots,n}$,
\\[0.2cm]
wobei $\delta_{i,j}$ das eben definierte Kronecker-Delta bezeichnet.  
Im Falle $n = 2$ haben wir beispielsweise
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{E}_2 := \left(
  \begin{array}[c]{ll}
    1 & 0 \\
    0 & 1 \\
  \end{array}
  \right)
$
\\[0.2cm]
und im Falle $n = 3$ gilt
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{E}_3 := \left(
  \begin{array}[c]{lll}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1 \\
  \end{array}
  \right)
$.
\\[0.2cm]
Ist $A \in \mathbb{K}^{m \times n}$,
so gilt
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \mathrm{E}_n = A$,
\\[0.2cm]
denn wenn wir $C := A \cdot \mathrm{E}_n$ definieren und in Komponentenschreibweise $C = (c_{i,j})$ haben, so
gilt nach der Definition der Matrizen-Multiplikation
\\[0.2cm]
\hspace*{1.3cm}
$c_{i,j} = \sum\limits_{k=1}^n a_{i,k} \cdot \delta_{k,j} = a_{i,j}$,
\\[0.2cm]
wobei wir bei der letzten Gleichung ausgenutzt haben, dass auf Grund der Definition des
Kronecker-Deltas von der Summe nur der Term mit $k = j$ \"{u}brig bleibt.  Genauso l\"{a}sst sich auch
zeigen, dass $\mathrm{E}_m \cdot A = A$ ist.

\begin{Definition}[Invertierbare Matrix]
  Eine quadratische Matrix $A \in \mathbb{K}^{n \times n}$ ist {\emph{\color{blue}invertierbar}} genau dann, wenn
  es eine Matrix $B \in \mathbb{K}^{n \times n}$ gibt, so dass 
  \\[0.2cm]
  \hspace*{1.3cm}
  $A \cdot B = B \cdot A = \mathrm{E}_n$
  \\[0.2cm]
  gilt.  In diesem Fall definieren wir $A^{-1} := B$ und sagen, dass $B$ das {\emph{\color{blue}Inverse}} der
  Matrix $A$ ist.  \qed
\end{Definition}

\remark
Wenn es eine Matrix $B$ gibt, so dass $A \cdot B = \mathrm{E}_n$ gilt, dann kann gezeigt werden, dass daraus
bereits $B \cdot A = \mathrm{E}_n$ folgt. \eox

\exercise
Es sei $A \in \mathbb{K}^{2 \times 2}$ gegeben als
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
       \begin{array}[c]{rr}
         a & b \\
         c & d 
       \end{array}
     \right)
$
\\[0.2cm]
und es gelte $a \cdot d \not= b \cdot c$.  Zeigen Sie, dass die Matrix $A$ ein Inverses hat und
berechnen Sie dieses Inverse.
Machen Sie dazu den Ansatz
\\[0.2cm]
\hspace*{1.3cm}
$A^{-1}  = \left(
       \begin{array}[c]{rr}
         e & f \\
         g & h 
       \end{array}
     \right)
$
\\[0.2cm]
und bestimmen Sie die Zahlen $e$, $f$, $g$ und $h$ aus der Forderung, dass $A^{-1} \cdot A = \mathrm{E}_2$
gelten muss.
\eoxs

\remark
Ein lineares Gleichungs-System mit $n$ Gleichungen und $n$ Variablen l\"{a}sst sich in der Form
$A \cdot \vec{x} = \vec{x}$ schreiben, wobei $A \in \mathbb{K}^{n \times n}$ ist.
  Beispielsweise k\"{o}nnen wir das lineare Gleichungs-System
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[c]{lllcl}
  1 \cdot x_1 + 2 \cdot x_2 + 3 \cdot x_3 = 4 \\ 
  5 \cdot x_1 + 6 \cdot x_2 + 7 \cdot x_3 = 8 \\ 
  9 \cdot x_1 + 6 \cdot x_2 + 3 \cdot x_3 = 1 \\ 
\end{array}
$
\\[0.2cm]
k\"{u}rzer als
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x} = \vec{b}$
\\[0.2cm]
schreiben, wenn wir
\\[0.2cm]
\hspace*{1.3cm}
$A := \left(
  \begin{array}[c]{lll}
    1 & 2 & 3 \\
    5 & 6 & 7 \\
    9 & 6 & 3 \\
  \end{array}
  \right)
$ \quad und \quad
$\vec{b} := \left(
  \begin{array}[c]{l}
    4 \\
    8 \\
    1 \\
  \end{array}
  \right)
$
\\[0.2cm]
definieren.  Wenn wir nun Gl\"{u}ck haben und die Matrix $A$ invertierbar ist, so k\"{o}nnen wir die
Gleichung $A \cdot \vec{x} = \vec{b}$ von links mit $A^{-1}$ multiplizieren.  Dann erhalten wir
\\[0.2cm]
\hspace*{1.3cm}
$A^{-1} \cdot A \cdot \vec{x} = A^{-1} \cdot \vec{b}$,
\\[0.2cm]
was wegen $A^{-1} \cdot A \cdot \vec{x} = \mathrm{E}_n \cdot \vec{x} = \vec{x}$ zu der Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} = A^{-1} \cdot \vec{b}$
\\[0.2cm]
\"{a}quivalent ist.  Falls wir also in der Lage sind, die Matrix $A$ zu invertieren,  dann k\"{o}nnen wir anschlie\3end
das L\"{o}sen der Gleichung $A \cdot \vec{x} = \vec{b}$  auf eine Matrix-Multiplikation zur\"{u}ck
f\"{u}hren.  Da das Inverse $A^{-1}$ nicht von dem Vektor $\vec{b}$ abh\"{a}ngt, ist dies dann ein
sinnvolles Vorgehen, wenn die Gleichung $A \cdot \vec{x} = \vec{b}$ f\"{u}r mehrere verschiedene Werte von
$\vec{b}$ gel\"{o}st werden soll.  \eoxs

\begin{Definition}
  Es sei $n\in\mathbb{N}$ und $k,l \in \{1,\cdots,n\}$.  Dann 
  definieren wir die $n \times n$ {\emph{\color{blue}Atomar-Matrix}} $\mathrm{A}(k,l)$ so, dass in Komponentenschreibweise
  \\[0.2cm]
  \hspace*{1.3cm}
  $\mathrm{A}_n(k,l) = (\delta_{i,k} \cdot \delta_{l,j})_{{i = 1, \cdots, n} \atop {j = 1, \cdots, n}}$
  \\[0.2cm]
  gilt.  Die Atomar-Matrix $\mathrm{A}(k,l)$ hat damit genau in der $k$-ten Zeile in der $l$-ten Spalte eine
  Eins und alle anderen Eintr\"{a}ge der Matrix sind Null.  \eoxs
\end{Definition}

\exercise
Es gelte $B \in \mathbb{K}^{n \times n}$ und $\mathrm{A}_n(k,l) \in \mathbb{K}^{n \times n}$ sei eine Atomar-Matrix.  
Wir definieren $C := \mathrm{A}_n(k,l) \cdot B$.  Zeigen Sie, dass in $C$ in genau einer Zeile von $0$ verschiedene
Werte auftreten.  Dies ist die $k$-te Zeile und die Werte, die hier auftreten, sind die Werte, die in
der $l$-ten Zeile von $B$ stehen.   Der Effekt der Multiplikation von $A(\mathrm{k},l)$ mit $B$ besteht also darin,
dass die $l$-te Zeile in die $k$-te Zeile verschoben wird und dass alle anderen Zeilen gel\"{o}scht
werden.  Schreiben wir die Matrix $B$ in der Form
\\[0.2cm]
\hspace*{1.3cm}
$B = \left(
  \begin{array}[c]{c}
    \vec{b}_1 \\
    \vdots       \\
    \vec{b}_n 
  \end{array} 
\right)
$, 
\\[0.2cm]
wobei $\vec{b}_i$ die $i$-te Zeile der Matrix $B$ bezeichnet, die wir als Vektor auffassen und
ist $l < k$, dann gilt also
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{A}_n(k,l) \cdot B = \mathrm{A}_n(k,l) \cdot \left(
  \begin{array}[c]{c}
    \vec{b}_1 \\
    \vdots       \\[0.1cm]
    \vec{b}_l \\
    \vdots       \\[0.1cm]
    \vec{b}_k \\
    \vdots       \\[0.1cm]
    \vec{b}_n 
  \end{array}
\right) = \left(
  \begin{array}[c]{c}
    \vec{0} \\
    \vdots       \\[0.1cm]
    \vec{0}   \\
    \vdots       \\[0.1cm]
    \vec{b}_l \\
    \vdots       \\[0.1cm]
    \vec{0} 
  \end{array}
\right),
$
\\[0.2cm]
falls $l < k$ ist.
\eox

Wir definieren nun drei verschiedene Arten von sogenannten {\emph{\color{blue}Elementar-Matrizen}}.  Die 
Zeilen-Additions-Matrizen $\mathrm{ZA}_n(k,l, \alpha)$ addieren das $\alpha$-fache der $l$-ten Zeile zur
$k$-ten Zeile, die Zeilen-Permutations-Matrizen $\mathrm{ZP}_n(k,l)$ vertauscht die $k$-te Zeile mit der
$l$-ten Zeile und schlie\3lich multipliziert die Zeilen-Multiplikations-Matrix $\mathrm{ZM}_n(k, \alpha)$  die $k$-te Zeile
mit $\alpha$.  

\begin{Definition}[Elementar-Matrizen]
  Es sei $n \in \mathbb{N}$.  Weiter seien $k,l \in \{1,\cdots,n\}$ und es sei
  $\alpha \in \mathbb{K}$, wobei zus\"{a}tzlich $\alpha \not= 0$ zu gelten hat.  Dann definieren wir die
  Elementar-Matrizen wie folgt:
  \begin{enumerate}
  \item F\"{u}r $k\not=l$ wird die $n \times n$ {\emph{\color{blue}Zeilen-Additions-Matrix}} $\mathrm{ZA}_n(k,l,\alpha)$ durch
        \\[0.2cm]
        \hspace*{1.3cm}
        $\mathrm{ZA}_n(k,l,\alpha) := \mathrm{E}_n + \alpha \cdot \mathrm{A}_n(k,l)$
        \\[0.2cm]
        definiert.  Multiplizieren wir $\mathrm{ZA}_n(k,l,\alpha)$ von rechts mit einer $n \times n$ Matrix
        $B$, so erhalten wir
        \\[0.2cm]
        \hspace*{1.3cm}
        $\mathrm{ZA}_n(k,l,\alpha) \cdot B := \mathrm{E}_n \cdot B + \alpha \cdot \mathrm{A}_n(k,l) \cdot B = B + \alpha \cdot \mathrm{A}_n(k,l) \cdot B$.
        \\[0.2cm]
        Die Matrix $\mathrm{A}_n(k,l) \cdot B$ besteht aus der $l$-ten Zeile von $B$, die allerdings
        in die $k$-te Zeile verschoben wird.  Damit besteht der Effekt der Multiplikation
        $\mathrm{ZA}_n(k,l,\alpha) \cdot B$ darin, dass die $l$-te Zeile von $B$  mit $\alpha$ multipliziert
        zur $k$-ten Zeile von $B$ hinzu addiert wird.  Im Falle $l < k$ gilt also
        \\[0.2cm]
        \hspace*{1.3cm}
        $\mathrm{ZA}_n(k,l,\alpha) \cdot B = \mathrm{ZA}_n(k,l,\alpha) \cdot \left(
          \begin{array}[c]{c}
            \vec{b}_1 \\
            \vdots       \\
            \vec{b}_l \\
            \vdots       \\
            \vec{b}_k \\
            \vdots       \\
            \vec{b}_n 
          \end{array}
        \right) = \left(
          \begin{array}[c]{c}
            \vec{b}_1 \\
            \vdots       \\
            \vec{b}_l   \\
            \vdots       \\
            \vec{b}_k + \alpha \cdot \vec{b}_l \\
            \vdots       \\
            \vec{b}_n 
          \end{array}
        \right).
        $       
\item Die $n \times n$ {\emph{\color{blue}Zeilen-Permutations-Matrix}} $\mathrm{ZP}_n(k,l)$ wird durch die Gleichung 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{ZP}_n(k,l) := \mathrm{E}_n + \mathrm{A}_n(k,l) + \mathrm{A}_n(l,k) - \mathrm{A}_n(k,k) - \mathrm{A}_n(l,l)$.
      \\[0.2cm]
      definiert.  Multiplizieren wir eine $n \times n$ Matrix $B$ von links mit
      $\mathrm{ZP}_n(k,l)$, so besteht der Effekt auf $B$ darin, dass die $k$-te Zeile von $B$ mit der
      $l$-ten Zeile von $B$ vertauscht wird, es gilt also
      \\[0.2cm]
      \hspace*{1.3cm}
        $\mathrm{ZP}_n(k,l) \cdot B = \mathrm{ZP}_n(k,l) \cdot \left(
          \begin{array}[c]{c}
            \vec{b}_1 \\
            \vdots       \\
            \vec{b}_l \\
            \vdots       \\
            \vec{b}_k \\
            \vdots       \\
            \vec{b}_n 
          \end{array}
        \right) = \left(
          \begin{array}[c]{c}
            \vec{b}_1 \\
            \vdots       \\
            \vec{b}_k \\
            \vdots       \\
            \vec{b}_l \\
            \vdots       \\
            \vec{b}_n 
          \end{array}
        \right).
        $       
      \\[0.2cm]
      Dies k\"{o}nnen wir wie folgt einsehen: 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{ZP}_n(k,l) \cdot B = B + \mathrm{A}_n(k,l) \cdot B+ \mathrm{A}_n(l,k) \cdot B - \mathrm{A}_n(k,k) \cdot B - \mathrm{A}_n(l,l) \cdot B$
      \\[0.2cm]
      Der Term $\mathrm{A}_n(k,l) \cdot B$ transportiert die $l$-Zeile von $B$ in die $k$-te Zeile, 
      der Term $\mathrm{A}_n(l,k) \cdot B$ transportiert die $k$-te Zeile von $B$ in
      die $l$-te Zeile und die beiden Terme $-\mathrm{A}_n(k,k) \cdot B$ und $-\mathrm{A}_n(l,l) \cdot B$ 
      entfernen die $k$-te Zeile beziehungsweise die $l$-te Zeile von $B$.
\item Die $n \times n$ {\emph{\color{blue}Zeilen-Multiplikations-Matrix}} $\mathrm{ZM}_n(k, \alpha)$ wird f\"{u}r $\alpha \not= 0$ durch die Gleichung
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{ZM}_n(k, \alpha) := \mathrm{E}_n + (\alpha - 1) \cdot \mathrm{A}_n(k, k) $
      \\[0.2cm]
      definiert.  Multiplizieren wir eine $n \times n$ Matrix $B$ von links mit
      $\textrm{ZM}_n(k,\alpha)$, so besteht der Effekt auf $B$ darin, dass die $k$-te Zeile von $B$
      mit $\alpha$ multipliziert wird, es gilt also
      \\[0.2cm]
      \hspace*{1.3cm}
        $\mathrm{ZM}_n(k,\alpha) \cdot B = \mathrm{ZM}_n(k,\alpha) \cdot \left(
          \begin{array}[c]{c}
            \vec{b}_1 \\
            \vdots       \\
            \vec{b}_k \\
            \vdots       \\
            \vec{b}_n 
          \end{array}
        \right) = \left(
          \begin{array}[c]{c}
            \vec{b}_1 \\
            \vdots       \\
            \alpha \cdot \vec{b}_k \\
            \vdots       \\
            \vec{b}_n 
          \end{array}
        \right).
        $       
      \\[0.2cm]
      Dies k\"{o}nnen wir wie folgt einsehen: 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{ZM}_n(k, \alpha) \cdot B = B + (\alpha - 1) \cdot \mathrm{A}_n(k,k) \cdot B$
      \\[0.2cm]
      Es wir also zu $B$ das $(1-\alpha)$-fache der $k$-ten Zeile von $B$ zur $k$-ten Zeile hinzu
      addiert und damit steht die $k$-te Zeile im Ergebnis insgesamt $1 + (\alpha-1) = \alpha$-mal.  \eoxs
 \end{enumerate}
\end{Definition}

Die entscheidende Beobachtung ist nun die, dass die oben definierten Elementar-Matrizen alle  invertierbar sind, im Einzelnen gilt:
\begin{enumerate}
\item $\bigl(\mathrm{ZA}_n(k,l,\alpha)\bigr)^{-1} =  \mathrm{ZA}_n(k,l,-\alpha)$,

      denn f\"{u}r $k \not= l$ zieht die Anwendung der Matrix $\mathrm{ZA}_n(k,l,-\alpha)$ auf eine
      Matrix $B$ das $\alpha$-fache der $l$-ten Zeile von der $k$-ten Zeile ab.  Wenn wir also das Produkt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{ZA}_n(k,l,-\alpha) \cdot \mathrm{ZA}_n(k,l,\alpha) \cdot \mathrm{E}_n$
      \\[0.2cm]
      betrachten, so addiert die Anwendung von $\mathrm{ZA}_n(k ,l,\alpha)$ auf die
      Einheits-Matrix $\mathrm{E}_n$ zun\"{a}chst das $\alpha$-fache der $l$-ten Zeile von 
      $\mathrm{E}_n$ zur $k$-ten Zeile, das dann durch die anschlie\3ende Anwendung von 
      $\mathrm{ZA}_n(k,l,-\alpha)$ wieder abgezogen wird.  Damit gilt insgesamt
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{ZA}_n(k,l,-\alpha) \cdot \mathrm{ZA}_n(k,l,\alpha) \cdot \mathrm{E}_n = \mathrm{E}_n$,
      \\[0.2cm]
      was wir zu
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{ZA}_n(k,l,-\alpha) \cdot \mathrm{ZA}_n(k,l,\alpha) = \mathrm{E}_n$,
      \\[0.2cm]
      vereinfachen k\"{o}nnen.  Genauso l\"{a}sst sich die Gleichung
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{ZA}_n(k,l,\alpha) \cdot \mathrm{ZA}_n(k,l,-\alpha) = \mathrm{E}_n$,
      \\[0.2cm]
      zeigen.
\item $\bigl(\mathrm{ZP}_n(k,l)\bigr)^{-1} = \mathrm{ZP}_n(k,l)$,

      denn wenn wir die $k$-te Zeile mit der $l$-ten Zeile vertauschen und in der resultierenden
      Matrix wieder die $k$-te Zeile mit der $l$-ten Zeile vertauschen, sind wir wieder bei der
      Matrix, mit der wir gestartet sind.
\item $\bigl(\mathrm{ZM}_n(k,\alpha)\bigr)^{-1} = \mathrm{ZM}_n(k,\alpha^{-1})$,

      denn wenn wir die $k$-te Zeile erst mit $\alpha$ multiplizieren und in der resultierenden
      Matrix  die $k$-te Zeile mit $\alpha^{-1}$ multiplizieren, haben wir die Matrix wegen der
      Gleichung  $\alpha^{-1} \cdot \alpha = 1$ insgesamt nicht ver\"{a}ndert.  Dabei muss nat\"{u}rlich
      $\alpha \not= 0$ gelten, aber das ist bei den Elementar-Matrizen der Form 
      $\mathrm{ZM}_n(k,\alpha)$ vorausgesetzt.
\end{enumerate}

\section{Berechnung des Inversen einer Matrix}
Wir haben nun alles Material zusammen um f\"{u}r eine gegebene Matrix $A \in \mathbb{K}^{n \times n}$
\"{u}berpr\"{u}fen zu k\"{o}nnen, ob es zu $A$ eine inverse Matrix $A^{-1}$ gibt und diese gegebenenfalls auch
zu berechnen.  Die Idee, die dem Verfahren, dass wir gleich pr\"{a}sentieren werden, zu Grunde liegt,
besteht darin, dass wir die Matrix $A$ solange mit Elementar-Matrizen $\mathrm{ZE}_i$
multiplizieren, bis wir die Matrix $A$ zur Einheits-Matrix $\mathrm{E}_n$ reduziert haben.  
Wir konstruieren also eine endliche Folge von Elementar-Matrizen 
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{ZE}_1$, $\cdots$, $\mathrm{ZE}_k$,
\\[0.2cm]
so, dass 
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{ZE}_k \cdot \mbox{\dots} \cdot \mathrm{ZE}_1 \cdot A = \mathrm{E}_n$
\\[0.2cm]
gilt, denn dann haben wir offenbar
\\[0.2cm]
\hspace*{1.3cm}
$A^{-1} = \mathrm{ZE}_k \cdot \mbox{\dots} \cdot \mathrm{ZE}_1$.
\\[0.2cm]
Die Elementar-Matrizen werden dabei nicht explizit berechnet, denn es reicht, wenn wir die
Umformungen, die den Elementar-Matrizen entsprechen, auf die Einheits-Matrix $\mathrm{E}_n$
anwenden.  Wir berechnen $A^{-1}$ dann nach der Formel
\\[0.2cm]
\hspace*{1.3cm}
$A^{-1} = \mathrm{ZE}_k \cdot \mbox{\dots} \cdot \mathrm{ZE}_1 \cdot \mathrm{E}_n$.
\\[0.2cm]
Bei dieser letzten Gleichung m\"{u}ssen wir keine Matrizen-Multiplikationen durchf\"{u}hren, denn wir wissen
ja, welchen Effekt die Multiplikation einer Elementar-Matrix $\mathrm{ZE}_i$ mit einer Matrix
$\mathrm{B}$ hat:  Je nachdem, um was f\"{u}r eine Elementar-Matrix es sich handelt, addieren wir das
$\alpha$-fache einer Zeile zu einer anderen Zeile, wir vertauschen zwei Zeilen oder wir
multiplizieren eine Zeile mit einem Skalar. 

Die Transformation der Matrix $A \in \mathbb{K}^n$ verl\"{a}uft in $n$ Schritten, wobei wir im $i$-ten
Schritt daf\"{u}r sorgen, dass die $i$-te Spalte der Matrix gleich dem $i$-ten Einheits-Vektor
$\vec{e}_i$ wird.  Dabei hat der $i$-te Einheits-Vektor in der $i$-ten Komponente eine $1$, alle
anderen Komponenten sind $0$.  Im $i$-ten Schritt k\"{o}nnen wir davon ausgehen, dass in den ersten $i-1$
Spalten bereits die Einheits-Vektoren $\vec{e}_1$ bis $\vec{e}_{i-1}$ stehen.
Um auch die $i$-te Spalte entsprechend zu transformieren, suchen wir zun\"{a}chst diejenige Zeile 
$j \in \{i, \cdots, n\}$ f\"{u}r die $|a_{j,i}|$ maximal wird.  Theoretisch w\"{u}rde es an dieser Stelle
reichen, einfach irgendeine Zeile $j \in \{i, \cdots, n\}$ zu w\"{a}hlen, f\"{u}r die $a_{j,i} \not= 0$ 
ist.  Das hier vorgestellte Verfahren, bei dem wir immer den Zeilen-Index $j$ bestimmen, f\"{u}r den
$a_{j,i}$ maximal ist, hat aber den Vorteil, dass es weniger anf\"{a}llig f\"{u}r Rundungs-Fehler ist als
das naive Vorgehen, bei dem $j$ einfach so gew\"{a}hlt wird, dass $a_{j,i} \not= 0$ ist.  Falls 
\\[0.2cm]
\hspace*{1.3cm}
$\forall j \in \{i, \cdots, n\} : a_{j,i} = 0$ 
\\[0.2cm]
gilt, dann l\"{a}sst sich die Matrix nicht mehr zur Einheits-Matrix umformen und es kann gezeigt werden,
dass die Matrix $A$ dann nicht invertierbar ist.  Falls wir einen Index $j$ wie oben beschrieben
finden k\"{o}nnen, dann vertauschen wir die $i$-te Zeile mit der $j$-ten Zeile und dividieren
anschlie\3end die $i$-te Zeile durch das Element $a_{i,i}$.  Damit steht in der $i$-ten Zeile der
$i$-ten Spalte schon mal eine $1$.  Damit die anderen Eintr\"{a}ge der $i$-ten Spalte $0$ werden,
subtrahieren wir anschlie\3end f\"{u}r alle $k \in \{ 1, \cdots, n \} \backslash \{i\}$ das $a_{k,i}$-fache der $i$-ten
Zeile von der $k$-ten Zeile.


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    inverse := procedure(a) {
        n := #a;              
        e := identity(n);
        for (i in [1 .. n]) {
            r := pivot(a, n, i);
            [ a[i], a[r] ] := [ a[r], a[i] ];   
            [ e[i], e[r] ] := [ e[r], e[i] ];
            if (a[i][i] == 0) { return; }
            f := 1 / a[i][i]; 
            for (j in [1 .. n]) {
                a[i][j] *= f; e[i][j] *= f;
            }
            for (k in [1 .. n] | k != i) {
                f := a[k][i];
                for (j in [1 .. n]) {
                    a[k][j] -= f * a[i][j];
                    e[k][j] -= f * e[i][j];
                }
            }    
        }
        return e;
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Berechnung der Inversen einer Matrix $a$.}
\label{fig:inverse.stlx}
\end{figure}

Abbildung \ref{fig:inverse.stlx} auf Seite \pageref{fig:inverse.stlx} zeigt eine Implementierung des oben beschriebenen Algorithmus in
der Sprache \href{http://randoom.org/Software/SetlX}{\textsc{SetlX}}, 
die leichter zu verstehen ist, als die oben gegebene rein textuelle Beschreibung.  Wir diskutieren
diese Implementierung jetzt Zeile f\"{u}r Zeile:
\begin{enumerate}
\item Die Funktion \texttt{inverse} bekommt als Argument eine quadratische Matrix \texttt{a}, die 
      als Liste von Listen dargestellt wird, es gilt also
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathtt{a} = [\mathtt{a}[1], \cdots, \mathtt{a}[n]]$.
      \\[0.2cm]
      Dabei ist $\mathtt{a}[i]$ gerade die $i$-te Zeile der Matrix $\mathtt{a}$.  Auf die Komponente
      $\mathtt{a}_{i,j}$ kann dann mittels des Ausdrucks $\mathtt{a}[i][j]$ zugegriffen werden.
\item Zun\"{a}chst wird in Zeile 2 mit Hilfe des Operators \texttt{\#}
      die Dimension $n$ der Matrix berechnet: Das ist gerade die L\"{a}nge der
      Liste \texttt{a}.
\item Das Prinzip der Berechnung der inversen Matrix $\mathtt{a}^{-1}$ ist, dass $\mathtt{a}$
      mittels elementarer Zeilen-Umformungen in die Einheits-Matrix $\mathrm{E}_n$ \"{u}berf\"{u}hrt wird.
      Gleichzeitig werden diese Umformungen auf die Einheits-Matrix $\mathrm{E}_n$ angewendet
      und am Ende enth\"{a}lt dann die umgeformte Einheits-Matrix das Inverse der Matrix $\mathtt{a}$.
 
      Zu diesem Zweck berechnen wir in Zeile 3 die Einheits-Matrix $\mathrm{E}_n$ mit Hilfe der
      Funktion \texttt{identity}.  Die Implementierung dieser Funktion wird in Abbildung
      \ref{fig:inverse.stlx-pivot} auf Seite \pageref{fig:inverse.stlx-pivot} gezeigt.
\item Im $i$-ten Durchlauf der Schleife in Zeile 4 wird erreicht, dass die $i$-te Spalte von
      $\mathtt{a}$ mit dem $i$-ten Einheits-Vektor $\vec{e}_i$ \"{u}bereinstimmt.
      \begin{enumerate}
      \item Zun\"{a}chst suchen wir mit der in Zeile $5$ aufgerufenen Funktion \texttt{pivot}
            nach der Zeile $r$, f\"{u}r die der Betrag $|\mathtt{a}[i][r]|$ unter allen 
            $r \in \{i, \cdots,n \}$ maximal wird.  Diese Zeile wird dann in Zeile 6 des Programms mit der $i$-ten Zeile
            vertauscht.  Anschlie\3end wird in Zeile 7 dieselbe Operation auf der Matrix $\mathtt{e}$ ausgef\"{u}hrt.

              Die Implementierung der dabei verwendeten Funktion $\mathtt{pivot}$ wird in Abbildung
              \ref{fig:inverse.stlx-pivot} gezeigt.
      \item Falls nach der Vertauschung von Zeile $i$ und Zeile $r$ die Komponente
            $\mathtt{a}[i][i]$ den Wert $0$ hat, dann haben alle
            Elemente in der $i$-ten Spalte ab dem Index $i$ den Wert $0$ und dann kann $\mathtt{a}$ nicht
            invertierbar sein.  Dies wird in Zeile 8 des Programms \"{u}berpr\"{u}ft: Falls 
            $\mathtt{a}[i][i] = 0$ ist, wird die Funktion abgebrochen, ohne ein Ergebnis zur\"{u}ck zu
            liefern. 
      \item Ansonsten teilen wir in Zeile 11 des Programms die $i$-Zeile von \texttt{a} und
            \texttt{e} durch $\mathtt{a}[i][i]$.  Anschlie\3end hat $a[i][i]$ nat\"{u}rlich den Wert $1$.
      \item In der Schleife in Zeile 13 des Programms subtrahieren wir das $\mathtt{a}[k][i]$-fache
            der $i$-ten Zeile von der $k$-ten Zeile.  Da die $i$-te Zeile an der Position $i$
            mittlerweile den Wert $1$ hat, f\"{u}hrt das dazu, dass danach der Wert von
            $\mathtt{a}[k][i]$ verschwindet, so dass in der $i$-ten Spalte alle Werte
            $\mathtt{a}[i][k]$ f\"{u}r $k \not= i$ gleich $0$ sind.
      \end{enumerate}
      Nachdem $i$ den Wert $n$ erreicht hat, ist die urspr\"{u}ngliche Matrix $\mathtt{a}$ zur
      Einheits-Matrix reduziert worden, sofern das Programm nicht vorher in Zeile 8 ergebnislos  
      abgebrochen wurde.  Gleichzeitig enth\"{a}lt dann die Variable $\mathtt{e}$ das Inverse der
      urspr\"{u}nglich gegebenen Matrix.
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    delta := procedure(i, j) {
        if (i == j) { return 1; } else { return 0; }
    };
    identity := procedure(n) {
        return [ [ delta(i,j) : j in [1 .. n] ] : i in [1 .. n] ];
    };
    pivot := procedure(a, n, i) {
        r := i;  // index of row containing maximal element
        for (j in [i+1 .. n]) {
            if (abs(a[j][i]) > abs(a[r][i])) {
                r := j;
            }
        }
        return r;
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Implementierung der in Abbildung \ref{fig:inverse.stlx} ben\"{o}tigten Hilfsfunktionen.}
\label{fig:inverse.stlx-pivot}
\end{figure}


\example
Wir testen das oben vorgestellte Verfahren, indem wir versuchen, das Inverse der Matrix
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}[c]{lll}
    1 & 2 & 3     \\
    2 & 3 & 1     \\
    3 & 1 & 2            
  \end{array}\right)
$
\\[0.2cm]
zu berechnen.  Dazu wird zun\"{a}chst die Variable $\mathtt{e}$ mit der Einheits-Matrix $\mathrm{E}_3$
initialisiert.  Danach haben wir
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{lll}
    1 & 2 & 3     \\
    2 & 3 & 1     \\
    3 & 1 & 2            
  \end{array}\right)
$
\quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{lll}
    1 & 0 & 0     \\
    0 & 1 & 0     \\
    0 & 0 & 1            
  \end{array}\right)
$
 \\[0.2cm]
\begin{enumerate}
\item Im ersten Schritt suchen wir die Zeile der Matrix $\mathtt{a}$, f\"{u}r die in der ersten Spalte
      das gr\"{o}\3te Element steht.  Im vorliegenden Fall ist das die dritte Zeile, denn es gilt $\mathtt{a}_{3,1} = 3$
      und alle anderen Eintr\"{a}ge in der ersten Spalte sind kleiner als $3$.  Daher vertauschen wir die
      erste Zeile mit der dritten Zeile und finden f\"{u}r die Matrizen $\mathtt{a}$ und $\mathtt{e}$
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{lll}
    3 & 1 & 2     \\
    2 & 3 & 1     \\
    1 & 2 & 3     
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{lll}
    0 & 0 & 1     \\
    0 & 1 & 0     \\
    1 & 0 & 0            
  \end{array}\right)
$
\\[0.2cm]

\item Im n\"{a}chsten Schritt normalisieren wir die erste Zeile, indem wir durch $\mathtt{a}_{1,1}$
      teilen.  Danach haben wir
      \\[0.2cm]
      \hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{lll}
    1 & \frac{1}{3} & \frac{2}{3} \\[0.2cm]
    2 &           3 &          1  \\[0.2cm]
    1 &           2 &          3     
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{lll}
    0 & 0 & \frac{1}{3}     \\
    0 & 1 &           0     \\
    1 & 0 &           0            
  \end{array}\right)
$
\\[0.2cm]
      Unser n\"{a}chstes Ziel ist es, alle Eintr\"{a}ge $\mathtt{a}_{i,1}$ in der ersten Spalte, f\"{u}r die 
      $i \not=1$ ist, durch Addition der ersten Zeile zur $i$-ten Zeile zu entfernen.  Dazu dienen
      die folgenden beiden Schritte:
\item Im n\"{a}chsten Schritt subtrahieren wir das $2$-fache der ersten Zeile von der zweiten Zeile
      und erhalten:
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{rrr}
    1 & \frac{1}{3} &  \frac{2}{3} \\[0.2cm]
    0 & \frac{7}{3} & -\frac{1}{3} \\[0.2cm]
    1 &           2 &          3     
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{rrr}
    0 & 0 &  \frac{1}{3}     \\[0.2cm]
    0 & 1 & -\frac{2}{3}     \\[0.2cm]
    1 & 0 &           0            
  \end{array}\right)
$
\item Jetzt subtrahieren wir die erste Zeile von der dritten Zeile:
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{rrr}
    1 & \frac{1}{3} &  \frac{2}{3} \\[0.2cm]
    0 & \frac{7}{3} & -\frac{1}{3} \\[0.2cm]
    0 & \frac{5}{3} &  \frac{7}{3}      
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{rrr}
    0 & 0 &  \frac{1}{3}     \\[0.2cm] 
    0 & 1 & -\frac{2}{3}     \\[0.2cm]
    1 & 0 & -\frac{1}{3}            
  \end{array}\right)
$
\item In den folgenden Schritten geht es darum, die zweite Spalte der Matrix $\mathtt{a}$ in den
      Einheits-Vektor $\vec{e}_2$ zu \"{u}berf\"{u}hren.  Zun\"{a}chst \"{u}berpr\"{u}fen wir, f\"{u}r welches $j \geq 2$
      der Betrag $|\mathtt{a}_{j,2}|$ maximal wird.  In diesem Fall ist der Betrag in der zweiten
      Zeile bereits maximal, so dass wir an dieser Stelle keine Zeilen-Vertauschung vornehmen m\"{u}ssen.
      Wir m\"{u}ssen die zweite Zeile aber noch normalisieren und multiplizieren daher die zweite Zeile
      mit $\frac{3}{7}$:
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{rrr}
    1 & \frac{1}{3} &  \frac{2}{3} \\[0.2cm]
    0 &           1 & -\frac{1}{7} \\[0.2cm]
    0 & \frac{5}{3} &  \frac{7}{3}      
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{rrr}
    0 &           0 &  \frac{1}{3}   \\[0.2cm] 
    0 & \frac{3}{7} & -\frac{2}{7}   \\[0.2cm]
    1 &           0 & -\frac{1}{3}            
  \end{array}\right)
$
\item Im n\"{a}chsten Schritt geht es darum, die Matrix $\mathtt{a}$ durch Addition eines geeigneten
      Vielfachen der zweiten Zeile zur ersten Zeile so zu ver\"{a}ndern, dass $\mathtt{a}_{2,1} = 0$
      wird.  Wir subtrahieren daher $\frac{1}{3}$ der zweiten Zeile von der ersten Zeile und
      erhalten 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{rrr}
    1 &           0 &  \frac{5}{7} \\[0.2cm]
    0 &           1 & -\frac{1}{7} \\[0.2cm]
    0 & \frac{5}{3} &  \frac{7}{3}      
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{rrr}
    0 & -\frac{1}{7} &  \frac{3}{7}   \\[0.2cm] 
    0 &  \frac{3}{7} & -\frac{2}{7}   \\[0.2cm]
    1 &            0 & -\frac{1}{3}            
  \end{array}\right)
$
\item Anschlie\3end subtrahieren wir $\frac{5}{3}$ der zweiten Zeile von der dritten Zeile und finden
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{rrr}
    1 &           0 &  \frac{5}{7} \\[0.2cm]
    0 &           1 & -\frac{1}{7} \\[0.2cm]
    0 &           0 &  \frac{18}{7}      
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{rrr}
    0 & -\frac{1}{7} &  \frac{3}{7}   \\[0.2cm] 
    0 &  \frac{3}{7} & -\frac{2}{7}   \\[0.2cm]
    1 & -\frac{5}{7} &  \frac{1}{7}            
  \end{array}\right)
$
\item In den verbleibenden Schritten geht es darum, die dritte Spalte der Matrix $\mathtt{a}$ in den
      Einheits-Vektor $\vec{e}_3$ zu \"{u}berf\"{u}hren.  Wir beginnen damit, dass wir die dritte Zeile
      durch $\frac{18}{7}$ teilen:
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{rrr}
    1 &           0 &  \frac{5}{7} \\[0.2cm]
    0 &           1 & -\frac{1}{7} \\[0.2cm]
    0 &           0 &            1      
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{rrr}
    0            & -\frac{1}{7}  &  \frac{3}{7}   \\[0.2cm] 
    0            &  \frac{3}{7}  & -\frac{2}{7}   \\[0.2cm]
    \frac{7}{18} & -\frac{5}{18} &  \frac{1}{18}            
  \end{array}\right)
$
\item Jetzt ziehen wir $\frac{5}{7}$ der dritten Zeile von der ersten Zeile ab:
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{rrr}
    1 &           0 &            0 \\[0.2cm]
    0 &           1 & -\frac{1}{7} \\[0.2cm]
    0 &           0 &            1      
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{rrr}
   -\frac{5}{18} &  \frac{1}{18} &  \frac{7}{18}  \\[0.2cm] 
    0            &  \frac{3}{7}  & -\frac{2}{7}   \\[0.2cm]
    \frac{7}{18} & -\frac{5}{18} &  \frac{1}{18}            
  \end{array}\right)
$
\item Im letzten Schritt addieren wir $\frac{1}{7}$ der dritten Zeile zur zweiten Zeile:
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} = \left(
  \begin{array}[c]{rrr}
    1 &           0 &            0 \\[0.2cm]
    0 &           1 &            0 \\[0.2cm]
    0 &           0 &            1      
  \end{array}\right)
$ \quad und \quad
$\mathtt{e} = \left(
  \begin{array}[c]{rrr}
   -\frac{5}{18} &  \frac{1}{18} &  \frac{7}{18}  \\[0.2cm] 
    \frac{1}{18} &  \frac{7}{18} & -\frac{5}{18}   \\[0.2cm]
    \frac{7}{18} & -\frac{5}{18} &  \frac{1}{18}            
  \end{array}\right)
$
\end{enumerate}
Damit haben wir das Inverse unserer urspr\"{u}nglichen Matrix gefunden, es gilt
\\[0.2cm]
\hspace*{1.3cm}
$\left(
  \begin{array}[c]{lll}
    1 & 2 & 3     \\
    2 & 3 & 1     \\
    3 & 1 & 2            
  \end{array}\right)^{-1} = \left(
  \begin{array}[c]{rrr}
   -\frac{5}{18} &  \frac{1}{18} &  \frac{7}{18}  \\[0.2cm] 
    \frac{1}{18} &  \frac{7}{18} & -\frac{5}{18}   \\[0.2cm]
    \frac{7}{18} & -\frac{5}{18} &  \frac{1}{18}            
  \end{array}\right)
$.
\eox


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "lineare-algebra"
%%% End: 
