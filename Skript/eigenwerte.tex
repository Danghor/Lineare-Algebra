\chapter{Eigenwerte und Eigenvektoren \label{chapter:eigenwerte}}
\href{https://de.wikipedia.org/wiki/Eigenwertproblem}{Eigenwerte und Eigenvektoren} geh\"oren zu den
wichtigsten Anwendungen der linearen Algebra: 
\begin{enumerate}
\item Die \href{http://de.wikipedia.org/wiki/Schr\"odingergleichung}{Schr\"odinger-Gleichung}
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathrm{H} \Psi = \mathrm{E} \cdot \Psi$
      \\[0.2cm]
      ist eine {\emph{\color{blue}Eigenwert-Gleichung}}.  Hier ist $\mathrm{H}$ der sogenannte
      \href{http://de.wikipedia.org/wiki/Hamilton-Operator}{Hamilton-Operator}, der auf die
      \href{http://de.wikipedia.org/wiki/Wellenfunktion}{Wellenfunktion} $\Psi$ angewendet wird.
      Dabei kommt wieder die Wellenfunktion $\Psi$ als Ergebnis heraus, allerdings multipliziert mit
      dem Eigenwert $\mathrm{E}$, der als die Energie der Wellenfunktion $\Psi$ interpretiert werden kann.

      Die Schr\"odinger-Gleichung ist die Grundlage der \href{http://de.wikipedia.org/wiki/Quantenmechanik}{Quanten-Mechanik}.
      Nat\"urlich erwarte ich von Ihnen nicht, dass Sie diese Gleichung verstehen.  Ich habe diese
      Gleichung als erstes aufgelistet,  weil ich im Rahmen meiner Diplomarbeit im
      wesentlichen nichts anderes gemacht habe, als auf numerischem Wege Schr\"odinger-Gleichungen zu
      l\"osen.  Dazu habe ich den Hamilton-Operator $\mathrm{H}$ zun\"achst durch eine Matrix
      approximiert.  Anschlie\3end konnte ich die Eigenwerte dieser Matrix berechnen.  
\item In der Informatik werden Eigenvektoren unter anderem bei der 
      {\emph{\color{blue}Unabh\"angigkeits-Analyse}} ben\"otigt.  Die Unabh\"angigkeits-Analyse
      kann beispielsweise dazu benutzt werden, das
      \href{http://research.ics.aalto.fi/ica/cocktail/cocktail_en.cgi}{Cocktail-Party-Problem} zu l\"osen.
      Bei diesem Problem geht es darum, verschiedene Ger\"auschquellen zu isolieren:  Stellen
      Sie sich vor, Sie sind auf einer Cocktail-Party.  Im Hintergrund spielt ein Klavier und Sie
      unterhalten sich mit einem Gespr\"achspartner.  Ihre Ohren h\"oren sowohl das Klavier als auch den
      Gespr\"achspartner, aber solange Sie noch nicht zu viele Cocktails getrunken haben, ist Ihr
      Gehirn in der Lage, das, was Ihr Gespr\"achspartner Ihnen erz\"ahlt, aus 
      dem Gesamtger\"ausch heraus zu filtern.  

      Neben dem Cocktail-Party-Problem gibt es zahlreiche anderen Anwendungen der
      Unabh\"angigkeits-Analyse sowohl in der Informatik im Bereich des 
      {\emph{\color{blue}Data-Minings}} als auch in vielen anderen Bereichen der Wissenschaft.
\item Eine andere Anwendung von Eigenvektoren ist die L\"osung von
      {\emph{\color{blue}Rekurrenz-Gleichungen}}, die bei Komplexit\"atsanalyse von Algorithmen
      auftreten.  Daf\"ur werden wir am Ende des Kapitels ein Beispiel geben.
\item Weitere Anwendungen von Eigenvektoren und den Ihnen zugeordneten Eigenwerten
      finden Sie beispielsweise unter 
      \\[0.2cm]
      \hspace*{1.3cm}
      \href{https://de.wikipedia.org/wiki/Eigenwertproblem#Praktische_Beispiele}{https://de.wikipedia.org/wiki/Eigenwertproblem\#Praktische\_Beispiele}.
\end{enumerate}
In diesem Kapitel f\"uhren wir zun\"achst Eigenwerte und Eigenvektoren ein und zeigen dann, wie sich 
lineare Rekurrenz-Gleichungen mit Hilfe von Eigenvektoren l\"osen lassen.  


\section{Definition und Berechnung von Eigenwerten}
\begin{Definition}[Eigenwert]
Es sei $A \in \mathbb{K}^{n \times n}$ eine quadratische Matrix.  Ein Vektor $\vec{x} \in \mathbb{K}^n$ mit $\vec{x} \not=\vec{0}$
ist ein {\emph{\color{blue}Eigenvektor}} der Matrix $A$ zum {\emph{\color{blue}Eigenwert}} $\lambda \in \mathbb{K}$ genau dann, wenn  
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x} = \lambda \cdot \vec{x}$
\\[0.2cm]
gilt.  \eoxs
\end{Definition}

\example
Die Matrix $A \in \mathbb{R}^{2 \times 2}$ sei als
\\[0.2cm]
\hspace*{1.3cm}
$A := \left(
  \begin{array}{ll}
    2 & 1 \\
    1 & 2
  \end{array}
  \right)
$
\\[0.2cm]
gegeben.  Die Vektoren $\vec{x}$ und $\vec{y}$ seien durch
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} := \left(
\begin{array}{r}
  1 \\
  1    
\end{array}\right)
$ \quad und \quad 
$\vec{y} := \left(
\begin{array}{r}
  1 \\
  -1    
\end{array}
\right)
$
\\[0.2cm]
definiert.  Dann gilt 
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x} = \left(
\begin{array}{r}
  3 \\
  3    
\end{array}\right) = 3 \cdot \vec{x}$ \quad und \quad
$A \cdot \vec{y} = \left(
\begin{array}{r}
  1 \\
  -1    
\end{array}\right) = 1 \cdot \vec{y}$.
\\[0.2cm]
Folglich ist $\vec{x}$ ein Eigenvektor von $A$ zum Eigenwert $3$, w\"ahrend $\vec{y}$ ein
Eigenvektor von $A$ zum Eigenwert $1$ ist.  \eox

\remark
Der Begriff des Eigenwerts und des Eigenvektors l\"asst sich auf beliebige lineare Operatoren
ausdehnen.  Ist $\mathrm{H} \in \mathcal{L}(V)$ ein Operator auf dem Vektor-Raum $V$, so ist
$\vec{x} \in V$ genau dann ein Eigenvektor zum Eigenwert $\lambda$, wenn 
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{H}(\vec{x}) = \lambda \cdot \vec{x}$
\\[0.2cm]
gilt.  Beispielsweise hat der Differential-Operator 
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{D}: \mathcal{C}^\infty(\mathbb{R}) \rightarrow  \mathcal{C}^\infty(\mathbb{R})$, 
\\[0.2cm]
der auf dem Raum 
\\[0.2cm]
\hspace*{1.3cm}
$\mathcal{C}^\infty(\mathbb{R}) := \bigl\{ f \in \mathbb{R}^\mathbb{R} \mid \mbox{$f$ ist beliebig oft  differenzierbar} \bigr\}$
\\[0.2cm]
durch die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\mathrm{D}(f) := \displaystyle\frac{\mathrm{d}f}{\mathrm{d}x}$
\\[0.2cm]
definiert ist, f\"ur jedes $\lambda \in \mathbb{R}$ die Funktion
\\[0.2cm]
\hspace*{1.3cm}
 $x \mapsto e^x$ 
\\[0.2cm]
als Eigenvektor zum Eigenwert $\lambda$.  
Der Vektor-Raum $\mathcal{C}^\infty(\mathbb{R})$ der unendlich oft differenzierbaren Funktionen ist
unendlich-dimensional und hat daher eine wesentlich komplexere Struktur als die
endlich-dimensionalen Vektor-R\"aume, mit denen wir uns im Rest dieser Vorlesung besch\"aftigen werden.
Da bei endlich-dimensionalen Vektor-R\"aumen jede lineare Abbildung durch eine Matrix dargestellt
werden kann, verlieren wir nichts, wenn wir uns auf die Bestimmung von Eigenvektoren und Eigenwerten
von Matrizen beschr\"anken. 
\eox

Der folgende Satz stellt einen Zusammenhang zwischen Eigenwerten und Determinanten her.  Dieser Satz
ist der Grund, warum wir gezwungen waren, den zugegebenerma\3en sehr komplexen Begriff der
Determinanten im letzten Kapitel einzuf\"uhren.

\begin{Satz}
  Es sei $A \in \mathbb{K}^{n \times n}$.  Dann ist $\lambda \in \mathbb{K}$ genau dann ein
  Eigenwert von $A$, wenn 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\mathtt{det}(\lambda \cdot \mathrm{E}_n - A) = 0$
  \\[0.2cm]
  gilt. 
\end{Satz}

\proof
Falls $\vec{x} \not= \vec{0}$ ein Eigenvektor von $A$ zum Eigenwert $\lambda$ ist, so haben wir die folgende
Kette von \"Aquivalenzen:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[b]{lll}
                  & \exists \vec{x} \in \mathbb{K}^n: \bigl(\vec{x} \not=\vec{0} \wedge A \cdot \vec{x}  = \lambda \cdot \vec{x}\bigr) \\[0.2cm]
\Leftrightarrow   & \exists \vec{x} \in \mathbb{K}^n: \bigl(\vec{x} \not=\vec{0} \wedge A \cdot \vec{x} - \lambda \cdot \mathrm{E}_n \cdot \vec{x} = \vec{0}\bigr) \\[0.2cm]
\Leftrightarrow   & \exists \vec{x} \in \mathbb{K}^n: \bigl(\vec{x} \not=\vec{0} \wedge (A - \lambda \cdot \mathrm{E}_n) \cdot \vec{x} = \vec{0}\bigr) \\[0.2cm]
\Leftrightarrow   & \mathtt{Kern}(A - \lambda \cdot \mathrm{E}_n) \not= \{\vec{0}\} \\[0.2cm]
\Leftrightarrow   & \mbox{$A - \lambda \cdot \mathrm{E}_n$ ist nicht invertierbar} \\[0.2cm]
\Leftrightarrow   & \mathtt{det}(A - \lambda \cdot \mathrm{E}_n) = 0  \\[0.2cm]
\Leftrightarrow   & \mathtt{det}(\lambda \cdot \mathrm{E}_n - A) = 0  
\end{array}
$
\qed

Nach dem letzten Satz sind die Eigenwerte genau die Werte $\lambda$, f\"ur die der Ausdruck 
\\[0.2cm]
\hspace*{1.3cm}
 $\mathtt{det}(\lambda \cdot \mathrm{E}_n- A)$
\\[0.2cm]
den Wert $0$ annimmt.  Berechnen wir diesen Ausdruck mit Hilfe der von Leibniz angegebenen Formel,
so erhalten wir ein Polynom in der Unbestimmten $\lambda$.  Dieses Polynom hei\3t das
{\emph{\color{blue}charakteristische Polynom}} der Matrix $A$ und ist formal als
\\[0.2cm]
\hspace*{1.3cm}
$\chi_A(\lambda) = \mathtt{det}(\lambda \cdot \mathrm{E}_n - A)$
\\[0.2cm]
definiert.  Die Nullstellen von $\chi_A$ sind also gerade die Eigenwerte von $A$, formal gilt
\\[0.2cm]
\hspace*{1.3cm}
$\lambda$ ist Eigenwert von $A$ \quad g.d.w. \quad $\chi_A(\lambda) = 0$.  
\\[0.2cm]
Falls $A \in \mathbb{K}^{n \times n}$ ist, so ist $\chi_A(\lambda)$ ein Polynom vom Grad $n$ in der Unbestimmten $\lambda$.


\example
Die Matrix $A \in \mathbb{R}^{2 \times 2}$ sei wie oben als
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{ll}
    2 & 1 \\
    1 & 2
  \end{array}
  \right)
$
\\[0.2cm]
gegeben.  Dann kann das charakteristische Polynom $\chi_A(\lambda)$ wie folgt berechnet werden:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  \chi_A(\lambda) & = & \mathtt{det}(\lambda \cdot \mathrm{E}_n - A) \\[0.2cm]
                  & = & \mathtt{det}\left(\begin{array}{cc}
                                          \lambda - 2 & -1          \\
                                                    -1 & \lambda - 2
                                          \end{array}
                        \right) \\[0.4cm]
                  & = & (\lambda - 2) \cdot (\lambda - 2) - (-1) \cdot (-1) \\[0.2cm]
                  & = & \lambda^2 - 4 \cdot \lambda + 4 - 1 \\[0.2cm]
                  & = & \lambda^2 - 4 \cdot \lambda + 3 \\[0.2cm]
                  & = & (\lambda - 3) \cdot (\lambda - 1). 
\end{array}
$
\\[0.2cm]
Offenbar hat dieses Polynom die Nullstellen $\lambda = 3$ und $\lambda = 1$.  Um beispielsweise den
Eigenvektor zum Eigenwert $\lambda = 3$ zu berechnen, m\"ussen wir die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$
\left(
  \begin{array}{ll}
    2 & 1 \\
    1 & 2 
  \end{array}
\right) \cdot \left(\begin{array}{l} x_1 \\ x_2 \end{array}\right) = 3 \cdot \left(\begin{array}{l} x_1 \\ x_2 \end{array}\right)
$
\\[0.2cm]
l\"osen.  Das liefert die beiden Gleichungen
\\[0.2cm]
\hspace*{1.8cm}
$2 \cdot x_1 + x_2 = 3 \cdot x_1$ \quad und \quad  $x_1 + 2 \cdot x_2 = 3 \cdot x_2$
\\
die wir zu
\\
\hspace*{1.8cm}
$-x_1 + x_2 = 0$ \quad und \quad  $x_1 - x_2 = 0$.
\\[0.2cm]
vereinfachen.
Offenbar ist die zweite Gleichung zur ersten Gleichung \"aquivalent und kann daher weggelassen
werden.  Da wir dann nur noch eine Gleichung aber zwei Unbekannte haben, k\"onnen wir eine der
Unbekannten frei w\"ahlen. Wie m\"ussen lediglich darauf achten, dass der Vektor
 $\ds\left(x_1 \atop x_2\right)$ vom Nullvektor verschieden ist.  Wir setzen daher $x_1 := 1$ und
 finden dann $x_2 = 1$.  Damit haben wir den Vektor $\ds\vec{x} = \left(1 \atop 1\right)$
als Eigenvektor der Matrix $A$ zum Eigenwert $3$ gefunden.

\exercise
\"Uberlegen Sie sich, f\"ur welche Werte von $a$, $b$ und $c$ die Matrix 
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{ll}
    a & b \\
    b & c
  \end{array}
\right)
$
\\[0.2cm]
zwei verschiedene reelle Eigenwerte besitzt. \eox

\exercise
F\"ur welche Werte von $\varphi$ hat die Matrix
\\[0.2cm]
\hspace*{1.3cm}
$A = \left(
  \begin{array}{rr}
    \cos(\varphi) & \sin(\varphi) \\
    -\sin(\varphi) & \cos(\varphi)
  \end{array}
\right)
$
\\[0.2cm]
keinen reellen Eigenwert? \eoxs

\begin{Definition}[Diagonalisierbarkeit] \hspace*{\fill} \\
Eine Matrix $A \in \mathbb{K}^{n \times n}$ ist {\emph{\color{blue}diagonalisierbar}} genau dann, wenn die Matrix
 $n$ linear unabh\"angige Eigenvektoren besitzt.  \eoxs 
\end{Definition}

\remark
Ist  $A \in \mathbb{K}^{n \times n}$ diagonalisierbar, so gibt es also $n$ verschiedene Vektoren
$\vec{x}_1, \cdots, \vec{x}_n$, so dass
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x}_i = \lambda_i \cdot \vec{x}_i$ \quad f\"ur alle $i=\{1,\cdots,n\}$
\\[0.2cm]
gilt. Fassen wir die $n$ Eigenvektoren $\vec{x}_i$ zu einer $n \times n$ Matrix $X$ zusammen, die
wir als
\\[0.2cm]
\hspace*{1.3cm}
$X = (\vec{x}_1, \cdots, \vec{x}_n)$
\\[0.2cm]
schreiben, wobei die $\vec{x}_i$ die Spalten der Matrix $X$ sind, so gilt
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot X = (\lambda_1 \cdot \vec{x}_1, \cdots, \lambda_n \cdot \vec{x}_n) = X \cdot D$,
\\[0.2cm]
wobei wir die Matrix $D$ als Diagonal-Matrix definieren, deren Diagonal-Elemente die Eigenwerte
$\lambda_i$ sind, w\"ahrend alle anderen Eintr\"age den Wert $0$ haben.  Damit gilt 
\\[0.2cm]
\hspace*{1.3cm}
$D = \left(
  \begin{array}{llllll}
    \lambda_1 & 0         & 0         & \cdots & 0 & 0 \\ 
    0         & \lambda_2 & 0         & \cdots & 0 & 0 \\
    0         & 0         & \lambda_3 & \cdots & 0 & 0 \\
    \vdots    & \vdots    & \vdots    & \ddots & \vdots & \vdots \\
    0         & 0         & 0         & \cdots  & \lambda_{n-1} & 0 \\
    0         & 0         & 0         & \cdots & 0 & \lambda_n 
  \end{array}
\right)
$.
\\[0.2cm]
In Komponentenschreibweise k\"onnen wir die Matrix $D$ als
\\[0.2cm]
\hspace*{1.3cm}
$D = \bigl(\lambda_i \cdot \delta_{i,j}\bigr)$ \quad f\"ur alle $i,j\in \{1,\cdots,n\}$
\\[0.2cm]
schreiben, wobei $\delta_{i,j}$ das bereits fr\"uher definierte
\href{http://de.wikipedia.org/wiki/Kronecker-Delta}{Kronecker-Delta} bezeichnet.  Da die $n$
Vektoren $\vec{ x}_1, \cdots, \vec{x}_n$ linear unabh\"angig sind, ist 
die Matrix $X = (\vec{x}_1, \cdots, \vec{x}_n)$ invertierbar.  Damit k\"onnen wir die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot X = X \cdot D$
\\[0.2cm]
von rechts mit der Matrix $X^{-1}$ multiplizieren und erhalten
\\[0.2cm]
\hspace*{1.3cm}
$A = X \cdot D \cdot X^{-1}$.
\\[0.2cm]
Dies ist n\"utzlich, wenn wir Potenzen der Matrix $A$ bilden wollen.  Beispielsweise gilt
\\[0.2cm]
\hspace*{1.3cm}
$A^2 = X \cdot D \cdot X^{-1} \cdot X \cdot D \cdot X^{-1} = X \cdot D^2 \cdot X^{-1}$
\\[0.2cm]
und durch eine einfache Induktion nach $k$ k\"onnen wir nach dem selben Prinzip zeigen, dass
\\[0.2cm]
\hspace*{1.3cm}
$A^k = X \cdot D^k \cdot X^{-1}$ \quad f\"ur alle $k \in \mathbb{N}$
\\[0.2cm]
gilt.  Diese Beobachtung hilft uns bei der Berechnung der Potenzen $A^k$, denn die Potenzen der
Diagonal-Matrix $D$ sind wesentlich  leichter zu berechnen sind als die Potenzen von $A$, es gilt
\\[0.2cm]
\hspace*{1.3cm}
$D^k = \left(
  \begin{array}{llllll}
    \lambda_1^k & 0         & 0         & \cdots & 0 & 0 \\ 
    0           & \lambda_2^k & 0         & \cdots & 0 & 0 \\
    0           & 0         & \lambda_3^k & \cdots & 0 & 0 \\
    \vdots      & \vdots    & \vdots    & \ddots & \vdots & \vdots \\
    0           & 0         & 0         & \cdots  & \lambda_{n-1}^k & 0 \\
    0           & 0         & 0         & \cdots & 0 & \lambda_n^k 
  \end{array}
\right)
$.
\\[0.2cm]
Diese Beobachtung wird uns im n\"achsten Abschnitt die explizite Berechnung der
\href{http://de.wikipedia.org/wiki/Fibonacci-Folge}{\emph{Fibonacci-Zahlen}} erm\"oglichen. 

\section{Die Berechnung der Fibonacci-Zahlen}
Die Folge $(f_n)_{n\in\mathbb{N}}$ der \href{http://de.wikipedia.org/wiki/Fibonacci-Folge}{Fibonacci-Zahlen} ist rekursiv durch die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$f_{n+2} = f_{n+1} + f_n$
\\[0.2cm]
zusammen mit den Anfangs-Bedingungen $f_0 = 0$ und $f_1 = 1$ definiert.   Eine Gleichung der obigen Form
wird als {\emph{\color{blue}lineare Rekurrenz-Gleichung}} bezeichnet.  Solche Gleichungen werden uns im zweiten
Semester bei der Absch\"atzung der Komplexit\"at von Algorithmen mehrfach begegnen und wir zeigen nun am Beispiel der
Fibonacci-Zahlen, wie sich eine solche Rekurrenz-Gleichung l\"osen l\"asst.  Dazu definieren wir
zun\"achst die Matrix $A$ als
\\[0.2cm]
\hspace*{1.3cm}
$A := \left(
  \begin{array}{ll}
    0 & 1 \\
    1 & 1 
  \end{array}
\right)
$.
\\
Weiter definieren wir den Vektor $\vec{x}_n$ f\"ur alle $n \in \mathbb{N}$ als 
$\ds\vec{x}_n := \left(f_n \atop f_{n+1}\right)$.  Dann haben wir
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{x}_n = \left(
  \begin{array}{ll}
    0 & 1 \\
    1 & 1 
  \end{array}
\right) \cdot \left(
  \begin{array}{c}
    f_n \\ f_{n+1}
  \end{array} 
\right) = 
\left(
  \begin{array}{c}
     f_{n+1} \\ f_n + f_{n+1}
  \end{array} 
\right) = \left(
  \begin{array}{c}
     f_{n+1} \\ f_{n+2}
  \end{array} 
\right) = \vec{x}_{n+1}.
$
\\[0.2cm]
Wir haben also gezeigt, dass
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x}_{n+1} = A \cdot \vec{x}_n$ \quad f\"ur alle $n \in \mathbb{N}$
\\[0.2cm]
gilt.  Aus dieser Gleichung $\vec{x}_{n+1} = A \cdot\vec{x}_n$ folgt durch eine einfache Induktion nach $n$, dass
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x}_n = A^n \cdot\vec{x}_0$
\\[0.2cm]
gilt, wobei $A^0$ die Einheits-Matrix $\mathrm{E}_2$ ist und $\ds\vec{x}_0 = \left(0 \atop 1\right)$ gilt.
Wollen wir die Potenzen $A^n$ berechnen, so ist es zweckm\"a\3ig, die Matrix $A$ vorher zu diagonalisieren.  Wir
berechnen zun\"achst das charakteristische Polynom von $A$:
\\[0.2cm]
\hspace*{1.3cm}
$\chi_A(\lambda) = \mathtt{det}(\lambda \cdot \mathrm{E}_2 - A) 
 = \mathtt{det}\left(
   \begin{array}{rr}
     \lambda  &  -1 \\
     -1       & \lambda - 1
   \end{array}
   \right) = \lambda \cdot (\lambda - 1) - 1 = \lambda^2 - \lambda - 1
$.
\\[0.2cm]
Nun bestimmen wir die Nullstellen von $\chi_A(\lambda)$, denn das sind die Eigenwerte von $A$:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lll}
                & \chi_A(\lambda) = 0 \\[0.2cm]
\Leftrightarrow & \lambda^2 - \lambda - 1 = 0 \\[0.2cm] 
\Leftrightarrow & \lambda^2 - \lambda     = 1 \\[0.2cm]
\Leftrightarrow & \lambda^2 - \lambda + \left(\frac{1}{2}\right)^2 = 1 + \frac{1}{4} 
                & \mbox{quadratische Erg\"anzung}  \\[0.4cm]
\Leftrightarrow & \ds\left(\lambda - \frac{1}{2}\right)^2 = \frac{5}{4} \\[0.4cm]
\Leftrightarrow & \ds\lambda = \frac{1}{2} + \frac{\sqrt{5}}{2} \;\vee\; \lambda = \frac{1}{2} - \frac{\sqrt{5}}{2} 
\end{array}
$
\\[0.2cm]
Wir definieren daher
\\[0.2cm]
\hspace*{1.3cm}
$\lambda_1 = \frac{1}{2}\cdot(1 + \sqrt{5})$ \quad und \quad $\lambda_2 = \frac{1}{2}\cdot(1 - \sqrt{5})$.
\\[0.2cm]
Als n\"achstes m\"ussen wir die Eigenvektoren bestimmen, die diesen beiden Eigenwerten zugeordnet sind.
Bezeichnen wir den Eigenvektor, der $\lambda_1$ zugeordnet ist, als $\ds\vec{y} := \left(y_1 \atop y_2\right)$, 
so haben wir also
\\[0.2cm]
\hspace*{1.3cm}
$A \cdot \vec{y} = \lambda_1 \cdot \vec{y}$
\\[0.2cm]
und aus dieser Gleichung folgt, dass f\"ur die Komponenten $y_1$ und $y_2$ die Gleichungen
\\[0.2cm]
\hspace*{1.3cm}
$y_2 = \lambda_1 \cdot y_1$  \quad und \quad $y_1 + y_2 = \lambda_1 \cdot y_2$
\\[0.2cm]
gelten m\"ussen.  Es l\"asst sich zeigen, dass die zweite Gleichung aus der ersten folgt.
Setzen wir $y_1 := 1$, so erhalten wir aus der ersten Gleichung $y_2 = \lambda_1$.
Damit hat der Eigenvektor $\vec{y}$ die Form
\\[0.2cm]
\hspace*{1.3cm}
$\ds\vec{y} = \left(1 \atop \lambda_1\right)$.
\\[0.2cm]
Auf analoge Weise finden wie f\"ur den zweiten Eigenvektor $\ds\vec{z} := \left(z_1 \atop z_2\right)$
das Ergebnis
\\[0.2cm]
\hspace*{1.3cm}
$\ds\vec{z} =  \left(1 \atop \lambda_2\right)$.
\\[0.2cm]
Damit hat die Matrix der Eigenvektoren die Form
\\[0.2cm]
\hspace*{1.3cm}
$X := \left(
  \begin{array}{ll}
            1 &         1 \\
    \lambda_1 & \lambda_2 
  \end{array}
  \right)
$.
\\[0.2cm]
Diese Matrix hat die Determinante $\mathtt{det}(X) = \lambda_2 - \lambda_1 = -\sqrt{5}$.
Wir hatten in einer \"Ubungsaufgabe eine Formel zur Berechnung des Inversen einer beliebigen $2 \times 2$ Matrix
hergeleitet.  Nach dieser Formel ist das Inverse der Matrix $X$ durch
\\[0.2cm]
\hspace*{1.3cm}
$X^{-1} = \bruch{1}{\mathtt{det}(A)} \cdot \left( 
  \begin{array}{rr}
    \lambda_2 & -1 \\
    -\lambda_1 & 1
  \end{array}
\right) = \bruch{1}{\sqrt{5}} \cdot \left( 
  \begin{array}{rr}
    -\lambda_2 & 1 \\
    \lambda_1 & -1
  \end{array}
\right) 
$
\\[0.2cm]
gegeben.  Damit gilt also
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
 \vec{x}_n & =&  A^n \cdot \left(
   \begin{array}{l}
    0 \\ 1     
   \end{array}
\right) = 
 \left(
  \begin{array}{ll}
            1 &         1 \\
    \lambda_1 & \lambda_2 
  \end{array}
  \right) \cdot
 \left(
  \begin{array}{ll}
    \lambda_1^n &       0 \\
             0  & \lambda_2^n 
  \end{array}
  \right) \cdot \bruch{1}{\sqrt{5}} \cdot \left( 
  \begin{array}{rr}
    -\lambda_2 & 1 \\
    \lambda_1 & -1
  \end{array}
\right) \cdot \left(
  \begin{array}{l}
     0 \\ 1    
  \end{array}
\right) \\[0.4cm]
   & = & \bruch{1}{\sqrt{5}} \cdot
 \left(
  \begin{array}{ll}
            1 &         1 \\
    \lambda_1 & \lambda_2 
  \end{array}
  \right) \cdot
 \left(
  \begin{array}{ll}
    \lambda_1^n &       0 \\
             0  & \lambda_2^n 
  \end{array}
  \right) \cdot \left(
    \begin{array}{r}
     1 \\ -1      
    \end{array}
\right) \\[0.4cm]
   & = & \bruch{1}{\sqrt{5}} \cdot
 \left(
  \begin{array}{ll}
            1 &         1 \\
    \lambda_1 & \lambda_2 
  \end{array}
  \right) \cdot
 \left(
  \begin{array}{r}
    \lambda_1^n  \\
   -\lambda_2^n 
  \end{array} 
  \right) \\[0.4cm]
   & = & \bruch{1}{\sqrt{5}} \cdot
 \left(
  \begin{array}{c}
    \lambda_1^n - \lambda_2^n \\[0.2cm]
   \lambda_1^{n+1} - \lambda_2^{n+1} 
  \end{array} 
  \right) 
\end{array}
$
\\[0.2cm]
Zwischen dem Vektor  $\vec{x}_n$ und den Fibonacci-Zahlen $f_n$ besteht nach Definition von $\vec{x}_n$ der Zusammenhang
\\[-0.2cm]
\hspace*{1.3cm}
$\ds \vec{x}_n = \left(
  \begin{array}{cc}
    f_n \\ f_{n+1}
  \end{array}
  \right)
$.
\\[0.2cm]
Daher haben wir f\"ur die $n$-te
Fibonacci-Zahl $f_n$ die Gleichung 
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{blue}{\framebox{\colorbox{yellow}{
$\ds f_n = \frac{1}{\sqrt{5}} \cdot (\lambda_1^n -\lambda_2^n)$ 
\quad mit \quad $\ds\lambda_1 = \frac{1}{2}\cdot \bigl(1 + \sqrt{5}\bigr)$ 
\quad und \quad $\ds\lambda_2 = \frac{1}{2}\cdot \bigl(1 - \sqrt{5}\bigr)$}}}}}
\\[0.2cm]
gefunden.

\exercise
L\"osen Sie die Rekurrenz-Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$a_{n+2} = 3 \cdot a_{n+1} - 2 \cdot a_n$ \quad f\"ur die Anfangs-Bedingungen $a_0 = 0$, $a_1 = 1$
\\[0.2cm]
mit Hilfe der in diesem Abschnitt vorgestellten Methode. \eox

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "lineare-algebra"
%%% End: 
